{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from statistical.data_availability import DataAvailability\n",
    "from statistical.data_retriever import DataRetriever\n",
    "from statistical.data_former import DataFormer\n",
    "from statistical.data_analyzer import DataAnalyzer\n",
    "\n",
    "from statistical.efficient_schedule_optimizer import ScheduleOptimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.date(2021, 9, 1), datetime.date(2021, 12, 24)), (datetime.date(2022, 1, 28), datetime.date(2022, 11, 16)), (datetime.date(2022, 11, 18), datetime.date(2023, 2, 24)), (datetime.date(2023, 3, 1), datetime.date(2023, 4, 19)), (datetime.date(2023, 4, 21), datetime.date(2023, 5, 31)), (datetime.date(2023, 6, 2), datetime.date(2023, 9, 14)), (datetime.date(2023, 11, 2), datetime.date(2024, 6, 11))]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Pick route and dates\n",
    "company = 'otraf'\n",
    "\n",
    "dave = DataAvailability()\n",
    "#continous_tripupd_periods = dave.find_continuous_periods('TripUpdates')\n",
    "cont_trip_upd = dave.find_continuous_periods('TripUpdates')\n",
    "\n",
    "print(cont_trip_upd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-09-01...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:04<00:00,  4.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-09-02...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:05<00:00,  5.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-09-03...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:04<00:00,  4.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-09-04...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:03<00:00,  3.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-09-05...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:04<00:00,  4.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-09-06...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:04<00:00,  4.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-09-07...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:05<00:00,  5.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-09-08...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:05<00:00,  5.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-09-09...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:04<00:00,  4.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-09-10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:04<00:00,  4.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-09-11...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:03<00:00,  3.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-09-12...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:03<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-09-13...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:04<00:00,  4.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-09-14...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:04<00:00,  4.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-09-15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:05<00:00,  5.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-09-16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:05<00:00,  5.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-09-17...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:05<00:00,  5.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-09-18...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:03<00:00,  3.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-09-19...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:03<00:00,  3.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-09-20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:04<00:00,  4.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-09-21...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:04<00:00,  4.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-09-22...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:06<00:00,  6.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-09-23...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:05<00:00,  5.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-09-24...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:05<00:00,  5.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-09-25...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:03<00:00,  3.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-09-26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:03<00:00,  3.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-09-27...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:04<00:00,  4.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-09-28...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:04<00:00,  4.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-09-29...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:04<00:00,  4.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-09-30...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:04<00:00,  4.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-10-01...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:04<00:00,  4.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-10-02...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:03<00:00,  3.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-10-03...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:03<00:00,  3.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-10-04...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:05<00:00,  5.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-10-05...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:04<00:00,  4.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-10-06...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:05<00:00,  5.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-10-07...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:04<00:00,  4.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-10-08...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:05<00:00,  5.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-10-09...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:03<00:00,  3.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-10-10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:03<00:00,  3.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-10-11...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:05<00:00,  5.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-10-12...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:05<00:00,  5.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-10-13...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using static data from 2021-10-12 instead of 2021-10-13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:02<00:00,  2.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-10-14...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:03<00:00,  4.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-10-15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:06<00:00,  6.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-10-16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:04<00:00,  4.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-10-17...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:03<00:00,  3.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-10-18...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:06<00:00,  6.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-10-19...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:05<00:00,  5.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-10-20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:06<00:00,  6.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-10-21...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:05<00:00,  5.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-10-22...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:05<00:00,  5.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-10-23...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using static data from 2021-10-22 instead of 2021-10-23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:03<00:00,  3.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-10-24...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:04<00:00,  4.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-10-25...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:04<00:00,  4.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-10-26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:04<00:00,  5.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-10-27...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:05<00:00,  5.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-10-28...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:05<00:00,  5.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-10-29...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:06<00:00,  6.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-10-30...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:04<00:00,  4.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-10-31...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:04<00:00,  4.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-11-01...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:06<00:00,  6.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-11-02...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:05<00:00,  5.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-11-03...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:05<00:00,  5.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-11-04...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:06<00:00,  6.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-11-05...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:06<00:00,  6.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-11-06...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:05<00:00,  5.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-11-07...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:05<00:00,  5.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-11-08...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:07<00:00,  7.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-11-09...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:06<00:00,  6.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-11-10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:06<00:00,  6.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-11-11...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:05<00:00,  5.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-11-12...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:05<00:00,  5.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-11-13...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:05<00:00,  5.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-11-14...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:04<00:00,  4.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-11-15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:05<00:00,  5.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-11-16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:05<00:00,  5.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-11-17...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:06<00:00,  6.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-11-18...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:06<00:00,  6.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-11-19...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:06<00:00,  6.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-11-20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:06<00:00,  6.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-11-21...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:05<00:00,  5.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-11-22...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:07<00:00,  7.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-11-23...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:06<00:00,  6.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-11-24...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:05<00:00,  5.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-11-25...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:06<00:00,  6.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-11-26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:06<00:00,  6.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-11-27...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:05<00:00,  5.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-11-28...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:05<00:00,  5.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-11-29...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:05<00:00,  5.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-11-30...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:06<00:00,  6.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-12-01...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:06<00:00,  6.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-12-02...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:06<00:00,  6.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-12-03...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:06<00:00,  6.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-12-04...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:05<00:00,  5.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-12-05...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:04<00:00,  4.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-12-06...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:06<00:00,  6.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-12-07...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:06<00:00,  6.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-12-08...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:06<00:00,  6.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-12-09...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:05<00:00,  5.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-12-10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:06<00:00,  6.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-12-11...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:05<00:00,  5.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-12-12...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:04<00:00,  4.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-12-13...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:06<00:00,  6.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-12-14...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:06<00:00,  6.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-12-15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:05<00:00,  5.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-12-16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:05<00:00,  5.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-12-17...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:05<00:00,  5.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-12-18...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:04<00:00,  4.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-12-19...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:04<00:00,  4.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-12-20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:05<00:00,  5.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-12-21...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:05<00:00,  5.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-12-22...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:05<00:00,  5.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-12-23...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:05<00:00,  5.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2021-12-24...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:02<00:00,  2.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records collected: 10969764\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data_chunks = []\n",
    "for (start_date,end_date) in cont_trip_upd:\n",
    "    start_date_str = start_date.strftime('%Y-%m-%d')\n",
    "    end_date_str = end_date.strftime('%Y-%m-%d')\n",
    "    data_retriever = DataRetriever(start_date=start_date_str,end_date=end_date_str)\n",
    "    data_chunks.append(data_retriever.data)\n",
    "    break\n",
    "\n",
    "raw_data_all = pd.concat(data_chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                                                  55700500756574033\n",
      "trip_id                                             55700000060411146\n",
      "start_date                                                   20210901\n",
      "schedule_relationship                                       SCHEDULED\n",
      "vehicle_id                                           9031005920505743\n",
      "timestamp                                                  1630475645\n",
      "stop_sequence                                                    10.0\n",
      "stop_id                                              9022005001319002\n",
      "arrival_delay                                                     4.0\n",
      "observed_arrival_time                                      1630475520\n",
      "departure_delay                                                  24.0\n",
      "observed_departure_time                                    1630475540\n",
      "arrival_uncertainty                                               0.0\n",
      "departure_uncertainty                                             0.0\n",
      "tripUpdate_stopTimeUpdate_scheduleRelationship                   None\n",
      "scheduled_arrival_time                            2021-09-01 07:51:56\n",
      "scheduled_departure_time                          2021-09-01 07:51:56\n",
      "stop_headsign                                             Resecentrum\n",
      "pickup_type                                                         3\n",
      "drop_off_type                                                       3\n",
      "shape_dist_traveled                                           3322.53\n",
      "timepoint                                                           0\n",
      "stop_name                                             Drottningtorget\n",
      "stop_lat                                                    58.410157\n",
      "stop_lon                                                    15.634508\n",
      "location_type                                                       0\n",
      "parent_station                                       9021005001319000\n",
      "platform_code                                                       B\n",
      "route_id                                             9011005021100000\n",
      "direction_id                                                        0\n",
      "service_id                                                         10\n",
      "trip_headsign                                                     NaN\n",
      "shape_id                                                          162\n",
      "agency_id                                           55700000000001407\n",
      "route_short_name                                                   11\n",
      "route_long_name                                                   NaN\n",
      "route_type                                                        700\n",
      "route_desc                                                        NaN\n",
      "datetime                                          2021-09-01 05:54:05\n",
      "date                                                       2021-09-01\n",
      "Name: 20000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Sätt pandas att visa alla kolumner och allt innehåll\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Visa första raden\n",
    "print(raw_data_all.iloc[20000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the replacement values first\n",
    "replacement_values = (\n",
    "    raw_data_all['route_id'].str[8:11]\n",
    ")\n",
    "\n",
    "# Fill only the NaN values\n",
    "raw_data_all['route_long_name'] = raw_data_all['route_long_name'].fillna(replacement_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['206' '202' '218' '213' '003' '002' 'Östgötapendeln' '201' '113' '110'\n",
      " '103' '204' '112' '211' '203' '482' '217' '111' '216' '416' '450' '661'\n",
      " '040' '205' '410' '030' '630' '459' '308' '432' '540' '440' '521' '433'\n",
      " '215' '613' '039' '058' '214' '522' '552' '655' '041' '520' '677' '539'\n",
      " '210' '631' '617' '616' '212' '620' '070' '413' '052' '623' '546' '429'\n",
      " '530' '046' '121' 'Kustpilen' '301' '615' '665' '566' '653' '612' '573'\n",
      " '045' '412' '417' '354' '303' '419' '484' '464' '535' '457' '513' '555'\n",
      " '074' '305' '486' '071' '681' '526' '545' '628' '572' '430' '355' '414'\n",
      " '302' '483' '221' '223' '574' '042' '059' '480' '444' '222' '571' '304'\n",
      " '629' '543' '120' '250' '075' '577' '471' '072' '065' '038' '670' '441'\n",
      " '232' '664' '663' '220' '249' '240' '243' '141' '241' '187' '452' '226'\n",
      " 'Servicelinje Kisa' '287' '356' '614' '538' '310' '309' '523'\n",
      " 'Anropsstyrd Stadstrafik Motala' 'Servicelinje Mjölby'\n",
      " 'Anropsstyrd Servicelinje']\n"
     ]
    }
   ],
   "source": [
    "print(raw_data_all['route_long_name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Route 201: 419861 rows\n",
      "Route 202: 191492 rows\n",
      "Route 203: 257029 rows\n",
      "Route 204: 253045 rows\n",
      "Route 205: 517714 rows\n",
      "Route 206: 249737 rows\n",
      "Route 210: 277573 rows\n",
      "Route 211: 158018 rows\n",
      "Route 212: 133296 rows\n",
      "Route 213: 315235 rows\n",
      "Route 214: 265883 rows\n",
      "Route 215: 159674 rows\n",
      "Route 216: 266034 rows\n",
      "Route 217: 197685 rows\n",
      "Route 218: 110204 rows\n",
      "Route 220: 9569 rows\n",
      "Route 221: 10010 rows\n",
      "Route 222: 19610 rows\n",
      "Route 226: 48661 rows\n"
     ]
    }
   ],
   "source": [
    "num_list = [201, 202, 203, 204, 205, 206, 210, 211, 212, 213, 214, 215, 216, 217, 218, 220, 221, 222, 226]\n",
    "test_dataframes = {}\n",
    "\n",
    "for i in num_list:\n",
    "    test_df = raw_data_all[raw_data_all['route_long_name'] == f'{i}'].copy()  # Using f-string instead\n",
    "    test_dataframes[i] = test_df\n",
    "    print(f\"Route {i}: {len(test_df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'type'>\n"
     ]
    }
   ],
   "source": [
    "import builtins\n",
    "str = builtins.str\n",
    "\n",
    "# Now test it\n",
    "print(type(str))  # Should show <class 'type'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hierarchical_stop_mappings(df):\n",
    "    \"\"\"\n",
    "    Build hierarchical mappings: stop_name -> parent_station -> stop_ids\n",
    "    \"\"\"\n",
    "    # Three-layer forward mapping\n",
    "    stop_name_to_parent_stations = {}\n",
    "    parent_station_to_stop_ids = {}\n",
    "    stop_name_to_stop_ids = {}  # Keep this for backward compatibility\n",
    "    \n",
    "    # Reverse mappings\n",
    "    stop_id_to_parent_station = {}\n",
    "    stop_id_to_stop_name = {}\n",
    "    parent_station_to_stop_name = {}\n",
    "    \n",
    "    for _, row in df[['stop_name', 'parent_station', 'stop_id']].drop_duplicates().iterrows():\n",
    "        stop_name = row['stop_name']\n",
    "        parent_station = str(row['parent_station'])\n",
    "        stop_id = str(row['stop_id'])\n",
    "        \n",
    "        # Layer 1: stop_name -> parent_stations\n",
    "        if stop_name not in stop_name_to_parent_stations:\n",
    "            stop_name_to_parent_stations[stop_name] = []\n",
    "        if parent_station not in stop_name_to_parent_stations[stop_name]:\n",
    "            stop_name_to_parent_stations[stop_name].append(parent_station)\n",
    "        \n",
    "        # Layer 2: parent_station -> stop_ids\n",
    "        if parent_station not in parent_station_to_stop_ids:\n",
    "            parent_station_to_stop_ids[parent_station] = []\n",
    "        if stop_id not in parent_station_to_stop_ids[parent_station]:\n",
    "            parent_station_to_stop_ids[parent_station].append(stop_id)\n",
    "        \n",
    "        # Flattened: stop_name -> stop_ids (for compatibility)\n",
    "        if stop_name not in stop_name_to_stop_ids:\n",
    "            stop_name_to_stop_ids[stop_name] = []\n",
    "        if stop_id not in stop_name_to_stop_ids[stop_name]:\n",
    "            stop_name_to_stop_ids[stop_name].append(stop_id)\n",
    "        \n",
    "        # Reverse mappings\n",
    "        stop_id_to_parent_station[stop_id] = parent_station\n",
    "        stop_id_to_stop_name[stop_id] = stop_name\n",
    "        parent_station_to_stop_name[parent_station] = stop_name\n",
    "    \n",
    "    # Sort for consistency\n",
    "    for stop_name in stop_name_to_parent_stations:\n",
    "        stop_name_to_parent_stations[stop_name].sort()\n",
    "    \n",
    "    for parent_station in parent_station_to_stop_ids:\n",
    "        parent_station_to_stop_ids[parent_station].sort()\n",
    "    \n",
    "    for stop_name in stop_name_to_stop_ids:\n",
    "        stop_name_to_stop_ids[stop_name].sort()\n",
    "    \n",
    "    return {\n",
    "        'hierarchical': {\n",
    "            'stop_name_to_parent_stations': stop_name_to_parent_stations,\n",
    "            'parent_station_to_stop_ids': parent_station_to_stop_ids,\n",
    "        },\n",
    "        'flattened': {\n",
    "            'stop_name_to_stop_ids': stop_name_to_stop_ids,\n",
    "        },\n",
    "        'reverse': {\n",
    "            'stop_id_to_parent_station': stop_id_to_parent_station,\n",
    "            'stop_id_to_stop_name': stop_id_to_stop_name,\n",
    "            'parent_station_to_stop_name': parent_station_to_stop_name,\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hierarchical_stop_mappings( df):\n",
    "    \"\"\"\n",
    "    Create two hierarchical dictionaries:\n",
    "    1. stop_name -> parent_station -> [stop_ids]\n",
    "    2. stop_id -> parent_station -> stop_name\n",
    "    \"\"\"\n",
    "\n",
    "    # Dictionary 1: stop_name -> parent_station -> [stop_ids]\n",
    "    stop_name_hierarchy = {}\n",
    "\n",
    "    # Dictionary 2: stop_id -> parent_station -> stop_name  \n",
    "    stop_id_hierarchy = {}\n",
    "\n",
    "    for _, row in df[['stop_name', 'parent_station', 'stop_id']].drop_duplicates().iterrows():\n",
    "        stop_name = row['stop_name']\n",
    "        parent_station = str(row['parent_station'])\n",
    "        stop_id = str(row['stop_id'])\n",
    "        \n",
    "        # Build Dictionary 1: stop_name -> parent_station -> [stop_ids]\n",
    "        if stop_name not in stop_name_hierarchy:\n",
    "            stop_name_hierarchy[stop_name] = {}\n",
    "        \n",
    "        if parent_station not in stop_name_hierarchy[stop_name]:\n",
    "            stop_name_hierarchy[stop_name][parent_station] = []\n",
    "        \n",
    "        if stop_id not in stop_name_hierarchy[stop_name][parent_station]:\n",
    "            stop_name_hierarchy[stop_name][parent_station].append(stop_id)\n",
    "        \n",
    "        # Build Dictionary 2: stop_id -> parent_station -> stop_name\n",
    "        stop_id_hierarchy[stop_id] = {\n",
    "            parent_station: stop_name\n",
    "        }\n",
    "\n",
    "    # Sort stop_ids for consistency\n",
    "    for stop_name in stop_name_hierarchy:\n",
    "        for parent_station in stop_name_hierarchy[stop_name]:\n",
    "            stop_name_hierarchy[stop_name][parent_station].sort()\n",
    "\n",
    "    return stop_name_hierarchy, stop_id_hierarchy\n",
    "\n",
    "# Usage\n",
    "stop_name_to_stops, stop_id_to_name = create_hierarchical_stop_mappings(test_dataframes[216])\n",
    "\n",
    "# Save to JSON\n",
    "import json\n",
    "\n",
    "with open('stop_name_to_stops_216.json', 'w') as f:\n",
    "    json.dump(stop_name_to_stops, f, indent=2)\n",
    "\n",
    "with open('stop_id_to_name_216.json', 'w') as f:\n",
    "    json.dump(stop_id_to_name, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'platforms': array(['9022005001394002', '9022005000226002', '9022005000416001',\n",
      "       '9022005000437002', '9022005001396001', '9022005001433001',\n",
      "       '9022005000422001', '9022005001435001', '9022005001445001',\n",
      "       '9022005001437002', '9022005001421002', '9022005001418001',\n",
      "       '9022005001419001', '9022005001323001', '9022005001426001',\n",
      "       '9022005000421001', '9022005001424001', '9022005001422001',\n",
      "       '9022005000198002', '9022005004186002', '9022005000412002',\n",
      "       '9022005001249002', '9022005001431002', '9022005001430002',\n",
      "       '9022005000042002', '9022005000050019', '9022005001243001',\n",
      "       '9022005000046003', '9022005000046006', '9022005001431001',\n",
      "       '9022005000412001', '9022005001430001', '9022005004186001',\n",
      "       '9022005000042001', '9022005000198001', '9022005001249001',\n",
      "       '9022005001243002', '9022005000050018', '9022005001422002',\n",
      "       '9022005001424002', '9022005000421002', '9022005001426002',\n",
      "       '9022005001323002', '9022005001418002', '9022005001419002',\n",
      "       '9022005000437001', '9022005000422002', '9022005001445002',\n",
      "       '9022005001435002', '9022005001421001', '9022005001396002',\n",
      "       '9022005001433002', '9022005001394001', '9022005001437001',\n",
      "       '9022005000226001'], dtype=object), 'parent_stations': array(['9021005001394000', '9021005000226000', '9021005000416000',\n",
      "       '9021005000437000', '9021005001396000', '9021005001433000',\n",
      "       '9021005000422000', '9021005001435000', '9021005001445000',\n",
      "       '9021005001437000', '9021005001421000', '9021005001418000',\n",
      "       '9021005001419000', '9021005001323000', '9021005001426000',\n",
      "       '9021005000421000', '9021005001424000', '9021005001422000',\n",
      "       '9021005000198000', '9021005004186000', '9021005000412000',\n",
      "       '9021005001249000', '9021005001431000', '9021005001430000',\n",
      "       '9021005000042000', '9021005000050000', '9021005001243000',\n",
      "       '9021005000046000'], dtype=object), 'standalone_stops': []}\n"
     ]
    }
   ],
   "source": [
    "def classify_stop_types(df):\n",
    "    \"\"\"\n",
    "    Classify stop_ids as either platforms of parent stations or parent stations themselves.\n",
    "    \n",
    "    Returns:\n",
    "    - platforms: stop_ids that are platforms (have a parent_station)\n",
    "    - parent_stations: stop_ids that are parent stations (appear in parent_station column)\n",
    "    - standalone_stops: stop_ids that are neither (standalone stops)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get unique stop information\n",
    "    stops_info = df[['stop_id', 'parent_station', 'location_type']].drop_duplicates()\n",
    "    \n",
    "    # Platforms: stop_ids that have a non-null parent_station\n",
    "    platforms = stops_info[stops_info['parent_station'].notna()]['stop_id'].unique()\n",
    "    \n",
    "    # Parent stations: stop_ids that appear in the parent_station column\n",
    "    parent_stations = stops_info['parent_station'].dropna().unique()\n",
    "    \n",
    "    # Standalone stops: stop_ids that are neither platforms nor parent stations\n",
    "    all_stop_ids = set(stops_info['stop_id'].unique())\n",
    "    platforms_set = set(platforms)\n",
    "    parent_stations_set = set(parent_stations)\n",
    "    \n",
    "    standalone_stops = all_stop_ids - platforms_set - parent_stations_set\n",
    "    \n",
    "    return {\n",
    "        'platforms': platforms,\n",
    "        'parent_stations': parent_stations, \n",
    "        'standalone_stops': list(standalone_stops)\n",
    "    }\n",
    "\n",
    "# Usage\n",
    "stop_classification = classify_stop_types(test_dataframes[216])\n",
    "print(stop_classification)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total counts for each stop_id + direction_id combination:\n",
      "            stop_id  direction_id  count\n",
      "0  9022005001437001             0    557\n",
      "1  9022005001437001             1   4783\n",
      "2  9022005001437002             0   4182\n"
     ]
    }
   ],
   "source": [
    "# Total counts for every stop_id + direction_id combination\n",
    "stop_direction_counts = test_dataframes[216].query(\"stop_name == 'Hackefors'\").groupby(['stop_id', 'direction_id']).size().reset_index(name='count')\n",
    "stop_direction_counts = stop_direction_counts.sort_values(['stop_id', 'direction_id'])\n",
    "print(\"Total counts for each stop_id + direction_id combination:\")\n",
    "print(stop_direction_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong combo 1 - scheduled times:\n",
      "4783 out of 4783\n",
      "Wrong combo 2 - scheduled times:\n",
      "4182 out of 4182\n"
     ]
    }
   ],
   "source": [
    "wrong_combo_1 = test_dataframes[216].query(\"stop_name == 'Hackefors' and stop_id == '9022005001437001' and direction_id == 1\")\n",
    "wrong_combo_2 = test_dataframes[216].query(\"stop_name == 'Hackefors' and stop_id == '9022005001437002' and direction_id == 0\")\n",
    "\n",
    "print(\"Wrong combo 1 - scheduled times:\")\n",
    "print(wrong_combo_1['scheduled_departure_time'].notna().sum(), \"out of\", len(wrong_combo_1))\n",
    "\n",
    "print(\"Wrong combo 2 - scheduled times:\")  \n",
    "print(wrong_combo_2['scheduled_departure_time'].notna().sum(), \"out of\", len(wrong_combo_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing raw data for route 201\n",
      "  Direction 0:\n",
      "    Most common start: Hässlegatan (9459 trips)\n",
      "    Most common end: Skäggetorp centrum (9477 trips)\n",
      "  Direction 1:\n",
      "    Most common start: Skäggetorp centrum (9762 trips)\n",
      "    Most common end: Hässlegatan (9697 trips)\n",
      "processing raw data for route 202\n",
      "  Direction 0:\n",
      "    Most common start: Tegskiftesgatan (7494 trips)\n",
      "    Most common end: Linköpings resecentrum (7502 trips)\n",
      "  Direction 1:\n",
      "    Most common start: Linköpings resecentrum (7363 trips)\n",
      "    Most common end: Tegskiftesgatan (7366 trips)\n",
      "processing raw data for route 203\n",
      "  Direction 0:\n",
      "    Most common start: Rydsvägens ändhållplats (9435 trips)\n",
      "    Most common end: Linköpings resecentrum (9445 trips)\n",
      "  Direction 1:\n",
      "    Most common start: Linköpings resecentrum (9004 trips)\n",
      "    Most common end: Rydsvägens ändhållplats (9011 trips)\n",
      "processing raw data for route 204\n",
      "  Direction 0:\n",
      "    Most common start: Landbogatan (7576 trips)\n",
      "    Most common end: Linköpings resecentrum (7579 trips)\n",
      "  Direction 1:\n",
      "    Most common start: Linköpings resecentrum (7405 trips)\n",
      "    Most common end: Landbogatan (7412 trips)\n",
      "processing raw data for route 205\n",
      "  Direction 0:\n",
      "    Most common start: Räknestickan (9467 trips)\n",
      "    Most common end: Skäggetorp centrum (9492 trips)\n",
      "  Direction 1:\n",
      "    Most common start: Skäggetorp centrum (9367 trips)\n",
      "    Most common end: Räknestickan (9394 trips)\n",
      "processing raw data for route 206\n",
      "  Direction 0:\n",
      "    Most common start: Fårullsvägen (7561 trips)\n",
      "    Most common end: Linköpings resecentrum (7571 trips)\n",
      "  Direction 1:\n",
      "    Most common start: Linköpings resecentrum (7327 trips)\n",
      "    Most common end: Fårullsvägen (7333 trips)\n",
      "processing raw data for route 210\n",
      "  Direction 1:\n",
      "    Most common start: Kråkbärsvägen (4949 trips)\n",
      "    Most common end: Roxtuna (4728 trips)\n",
      "  Direction 0:\n",
      "    Most common start: Roxtuna (4761 trips)\n",
      "    Most common end: Kråkbärsvägen (4694 trips)\n",
      "processing raw data for route 211\n",
      "  Direction 0:\n",
      "    Most common start: Övre Johannelund (6224 trips)\n",
      "    Most common end: Linköpings resecentrum (6226 trips)\n",
      "  Direction 1:\n",
      "    Most common start: Linköpings resecentrum (6040 trips)\n",
      "    Most common end: Övre Johannelund (6042 trips)\n",
      "processing raw data for route 212\n",
      "  Direction 1:\n",
      "    Most common start: Linköpings resecentrum (3996 trips)\n",
      "    Most common end: Änggårdsskolan (4001 trips)\n",
      "  Direction 0:\n",
      "    Most common start: Änggårdsskolan (3839 trips)\n",
      "    Most common end: Linköpings resecentrum (3842 trips)\n",
      "processing raw data for route 213\n",
      "  Direction 0:\n",
      "    Most common start: Nedre Johannelund (6256 trips)\n",
      "    Most common end: Fönvindsvägen östra (6275 trips)\n",
      "  Direction 1:\n",
      "    Most common start: Fönvindsvägen östra (6171 trips)\n",
      "    Most common end: Nedre Johannelund (6182 trips)\n",
      "processing raw data for route 214\n",
      "  Direction 0:\n",
      "    Most common start: Aspnäset (4831 trips)\n",
      "    Most common end: Gamla Linköping (4764 trips)\n",
      "  Direction 1:\n",
      "    Most common start: Gamla Linköping (4688 trips)\n",
      "    Most common end: Aspnäset (4699 trips)\n",
      "processing raw data for route 215\n",
      "  Direction 0:\n",
      "    Most common start: Klarinettgatan (4762 trips)\n",
      "    Most common end: Linköpings resecentrum (4765 trips)\n",
      "  Direction 1:\n",
      "    Most common start: Linköpings resecentrum (4689 trips)\n",
      "    Most common end: Klarinettgatan (4691 trips)\n",
      "processing raw data for route 216\n",
      "  Direction 0:\n",
      "    Most common start: Aspnäset (4729 trips)\n",
      "    Most common end: Berga centrum (4816 trips)\n",
      "  Direction 1:\n",
      "    Most common start: Berga centrum (4735 trips)\n",
      "    Most common end: Aspnäset (4743 trips)\n",
      "processing raw data for route 217\n",
      "  Direction 0:\n",
      "    Most common start: Aspnäset (4702 trips)\n",
      "    Most common end: Linköpings resecentrum (4709 trips)\n",
      "  Direction 1:\n",
      "    Most common start: Linköpings resecentrum (4537 trips)\n",
      "    Most common end: Aspnäset (4542 trips)\n",
      "processing raw data for route 218\n",
      "  Direction 0:\n",
      "    Most common start: Gamla Linköping (4718 trips)\n",
      "    Most common end: Linköpings resecentrum (4725 trips)\n",
      "  Direction 1:\n",
      "    Most common start: Linköpings resecentrum (4625 trips)\n",
      "    Most common end: Gamla Linköping (4628 trips)\n",
      "processing raw data for route 220\n",
      "  Direction 1:\n",
      "    Most common start: Linköpings resecentrum (705 trips)\n",
      "    Most common end: Mjärdevi (706 trips)\n",
      "  Direction 0:\n",
      "    Most common start: Mjärdevi (889 trips)\n",
      "    Most common end: Linköpings resecentrum (889 trips)\n",
      "processing raw data for route 221\n",
      "  Direction 1:\n",
      "    Most common start: Linköpings resecentrum (703 trips)\n",
      "    Most common end: Landerydsvägen (704 trips)\n",
      "  Direction 0:\n",
      "    Most common start: Landerydsvägen (725 trips)\n",
      "    Most common end: Linköpings resecentrum (727 trips)\n",
      "processing raw data for route 222\n",
      "  Direction 1:\n",
      "    Most common start: Ekhaga (391 trips)\n",
      "    Most common end: Mjärdevi (392 trips)\n",
      "  Direction 0:\n",
      "    Most common start: Mjärdevi (323 trips)\n",
      "    Most common end: Ekhaga (323 trips)\n",
      "processing raw data for route 226\n",
      "  Direction 1:\n",
      "    Most common start: MAXI (1028 trips)\n",
      "    Most common end: Landbogatan (1029 trips)\n",
      "  Direction 0:\n",
      "    Most common start: Landbogatan (916 trips)\n",
      "    Most common end: MAXI (919 trips)\n",
      "\n",
      "Route terminals data saved to route_terminals.json\n",
      "Processed 19 routes\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "route_terminals = {}\n",
    "\n",
    "for i in num_list:\n",
    "    print(f'processing raw data for route {i}')\n",
    "    test_df = test_dataframes[i]\n",
    "    \n",
    "    # Get route name for the key\n",
    "    route_name = f'{i}'\n",
    "    route_terminals[route_name] = {}\n",
    "    \n",
    "    # Get most common start and end points for each direction\n",
    "    for direction in test_df['direction_id'].unique():\n",
    "        dir_data = test_df[test_df['direction_id'] == direction]\n",
    "        \n",
    "        # Get start points (first stop of each trip) - using transform to avoid grouping column issues\n",
    "        start_points = dir_data.loc[dir_data.groupby(['trip_id', 'start_date'])['stop_sequence'].idxmin(), 'stop_name'].value_counts()\n",
    "        \n",
    "        # Get end points (last stop of each trip) - using transform to avoid grouping column issues  \n",
    "        end_points = dir_data.loc[dir_data.groupby(['trip_id', 'start_date'])['stop_sequence'].idxmax(), 'stop_name'].value_counts()\n",
    "        \n",
    "        most_common_start = start_points.index[0] if len(start_points) > 0 else \"No data\"\n",
    "        most_common_end = end_points.index[0] if len(end_points) > 0 else \"No data\"\n",
    "        \n",
    "        # Store in the dictionary - convert direction to string\n",
    "        route_terminals[route_name][str(direction)] = {\n",
    "            'start': most_common_start,\n",
    "            'end': most_common_end\n",
    "        }\n",
    "        \n",
    "        print(f\"  Direction {direction}:\")\n",
    "        print(f\"    Most common start: {most_common_start} ({start_points.iloc[0] if len(start_points) > 0 else 0} trips)\")\n",
    "        print(f\"    Most common end: {most_common_end} ({end_points.iloc[0] if len(end_points) > 0 else 0} trips)\")\n",
    "\n",
    "# Dump to JSON\n",
    "with open('route_terminals.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(route_terminals, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nRoute terminals data saved to route_terminals.json\")\n",
    "print(f\"Processed {len(route_terminals)} routes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''okay so this information should be converted to a regulation json on the finest granularity format. regulation[composite_key]={'route_name': , 'stop_name': 'regulations':{}, regulation[composite_key]['regulations'][sub_key]={'direction_id': , 'time_type': , 'regulation stop': True if composite key is a regulatory stop in that direction else False , 'multiple_stop_sequences': True'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing raw data for route 201\n",
      "Removing 0 duplicates.\n",
      "\n",
      "=== STOP TOPOLOGY VALIDATION ===\n",
      "Built mapping for 23 stop names\n",
      "✅ Hässlegatan: Valid mapping\n",
      "✅ Myntgatan: Valid mapping\n",
      "✅ Grindgatan: Valid mapping\n",
      "✅ Resedan: Valid mapping\n",
      "✅ Berga Söderleden: Valid mapping\n",
      "✅ Ridhusgatan: Valid mapping\n",
      "✅ Middagsgatan: Valid mapping\n",
      "✅ Strågatan: Valid mapping\n",
      "✅ Majelden: Valid mapping\n",
      "✅ Rusthållaregården: Valid mapping\n",
      "✅ Mellangården: Valid mapping\n",
      "✅ Skäggetorp centrum: Valid mapping\n",
      "✅ Blomgatan: Valid mapping\n",
      "✅ Folkungavallen: Valid mapping\n",
      "✅ Klostergatan: Valid mapping\n",
      "✅ Åbylund: Valid mapping\n",
      "✅ Abisko: Valid mapping\n",
      "✅ Konsert & Kongress: Valid mapping\n",
      "✅ Tinnerbäcksbadet: Valid mapping\n",
      "✅ Trädgårdstorget: Valid mapping\n",
      "✅ Kungsgatan: Valid mapping\n",
      "✅ Linköpings resecentrum: Valid mapping\n",
      "✅ Bergdalsgatan: Valid mapping\n",
      "Validation complete: 0 stop_id-direction violations detected (detailed logging deferred)\n",
      "\n",
      "=== UNIFIED TRIP CLASSIFICATION WITH GAP DETECTION ===\n",
      "Direction 1: canonical pattern 22 stops (from 8377 trips)\n",
      "Direction 0: canonical pattern 22 stops (from 8184 trips)\n",
      "Complete: 19252 trips, 6 stop-level violations affecting 54357 records\n",
      "Additional info: 55325 total non-full trip records\n",
      "\n",
      "=== FINALIZING TOPOLOGY VIOLATIONS LOG ===\n",
      "Finalized 0 detailed topology violations with trip_type granularity\n",
      "\n",
      "=== DETECTING REGULATORY STOPS ===\n",
      "Found 59 regulatory combinations\n",
      "Regulatory violations created: 0\n",
      "Regulatory analysis complete: 59 combinations, 0 violations\n",
      "\n",
      "=== CALCULATING TRAVEL TIMES AND DELAYS ===\n",
      "  Using stop-level pattern violations for travel time validation\n",
      "Calculation complete: 397824/419667 valid segments (94.8%)\n",
      "  Using stop-level pattern violations for precise validation\n",
      "\n",
      "=== CREATING MASTER INDEXER ===\n",
      "Processing 328 unique combinations\n",
      "Master indexer created: 328 combinations\n",
      "  - Flagged: 74 (22.6%)\n",
      "  - High severity: 0\n",
      "  - Medium severity: 74\n",
      "  - Low severity: 0\n",
      "\n",
      "=== CREATING NEW NAVIGATION MAPS ===\n",
      "Using topology validation results for stop mapping\n",
      "Processing 748 unique data combinations\n",
      "Navigation maps created:\n",
      "  - 42 stops in combinations\n",
      "  - 1 routes\n",
      "  - 23 stop names mapped\n",
      "\n",
      "=== EXPORTING ALL DATA WITH GLOBAL MERGING ===\n",
      "Using global output folder: transit_analysis_global\n",
      "✅ master_indexer.json: Created new file (328 entries)\n",
      "✅ log_pattern_violations.json: Created new file (6 entries)\n",
      "✅ log_trip_types.json: Created new file (328 entries)\n",
      "✅ log_regulatory_stops.json: Created new file (59 entries)\n",
      "✅ global_stop_to_combinations.json: Created global navigation file\n",
      "✅ global_route_to_combinations.json: Created global navigation file\n",
      "✅ global_stop_name_to_stop_ids.json: Created global navigation file\n",
      "✅ global_route_short_name_to_info.json: Created global navigation file\n",
      "✅ Global summary updated: transit_analysis_global\\global_summary.json\n",
      "\n",
      "🌍 GLOBAL ANALYSIS STATUS:\n",
      "  - Routes analyzed: 1\n",
      "  - Stop names: 23\n",
      "  - Total combinations: 328\n",
      "  - Issues detected: Yes\n",
      "  - Global files: transit_analysis_global\n",
      "  - Use master_indexer.json for detailed violation analysis\n",
      "processing raw data for route 202\n",
      "Removing 0 duplicates.\n",
      "\n",
      "=== STOP TOPOLOGY VALIDATION ===\n",
      "Built mapping for 14 stop names\n",
      "✅ Tegskiftesgatan: Valid mapping\n",
      "✅ Odalgatan: Valid mapping\n",
      "✅ Bygdegatan: Valid mapping\n",
      "✅ Hemmansgatan: Valid mapping\n",
      "✅ Djurgården centrum: Valid mapping\n",
      "✅ Infanterivägen: Valid mapping\n",
      "✅ US Södra entrén: Valid mapping\n",
      "✅ US Norra entrén: Valid mapping\n",
      "✅ Tinnerbäcksbadet: Valid mapping\n",
      "✅ Djurgården: Valid mapping\n",
      "✅ Trädgårdstorget: Valid mapping\n",
      "✅ Kungsgatan: Valid mapping\n",
      "✅ Linköpings resecentrum: Valid mapping\n",
      "✅ Garnisonen: Valid mapping\n",
      "Validation complete: 0 stop_id-direction violations detected (detailed logging deferred)\n",
      "\n",
      "=== UNIFIED TRIP CLASSIFICATION WITH GAP DETECTION ===\n",
      "Direction 1: canonical pattern 13 stops (from 5499 trips)\n",
      "Direction 0: canonical pattern 13 stops (from 5601 trips)\n",
      "Complete: 14868 trips, 5 stop-level violations affecting 46869 records\n",
      "Additional info: 46934 total non-full trip records\n",
      "\n",
      "=== FINALIZING TOPOLOGY VIOLATIONS LOG ===\n",
      "Finalized 0 detailed topology violations with trip_type granularity\n",
      "\n",
      "=== DETECTING REGULATORY STOPS ===\n",
      "Found 28 regulatory combinations\n",
      "Regulatory violations created: 0\n",
      "Regulatory analysis complete: 28 combinations, 0 violations\n",
      "\n",
      "=== CALCULATING TRAVEL TIMES AND DELAYS ===\n",
      "  Using stop-level pattern violations for travel time validation\n",
      "Calculation complete: 170712/191234 valid segments (89.3%)\n",
      "  Using stop-level pattern violations for precise validation\n",
      "\n",
      "=== CREATING MASTER INDEXER ===\n",
      "Processing 124 unique combinations\n",
      "Master indexer created: 124 combinations\n",
      "  - Flagged: 46 (37.1%)\n",
      "  - High severity: 0\n",
      "  - Medium severity: 46\n",
      "  - Low severity: 0\n",
      "\n",
      "=== CREATING NEW NAVIGATION MAPS ===\n",
      "Using topology validation results for stop mapping\n",
      "Processing 406 unique data combinations\n",
      "Navigation maps created:\n",
      "  - 27 stops in combinations\n",
      "  - 1 routes\n",
      "  - 14 stop names mapped\n",
      "\n",
      "=== EXPORTING ALL DATA WITH GLOBAL MERGING ===\n",
      "Using global output folder: transit_analysis_global\n",
      "✅ master_indexer.json: Route cleanup + merge (328 → 452, +124 new)\n",
      "✅ log_pattern_violations.json: Route cleanup + merge (6 → 11, +5 new)\n",
      "✅ log_trip_types.json: Route cleanup + merge (328 → 452, +124 new)\n",
      "✅ log_regulatory_stops.json: Route cleanup + merge (59 → 87, +28 new)\n",
      "✅ global_stop_to_combinations.json: Global navigation accumulated (multi-route)\n",
      "✅ global_route_to_combinations.json: Global navigation accumulated (multi-route)\n",
      "✅ global_stop_name_to_stop_ids.json: Global navigation accumulated (multi-route)\n",
      "✅ global_route_short_name_to_info.json: Global navigation accumulated (multi-route)\n",
      "✅ Global summary updated: transit_analysis_global\\global_summary.json\n",
      "\n",
      "🌍 GLOBAL ANALYSIS STATUS:\n",
      "  - Routes analyzed: 2\n",
      "  - Stop names: 37\n",
      "  - Total combinations: 452\n",
      "  - Issues detected: Yes\n",
      "  - Global files: transit_analysis_global\n",
      "  - Use master_indexer.json for detailed violation analysis\n",
      "processing raw data for route 203\n",
      "Removing 0 duplicates.\n",
      "\n",
      "=== STOP TOPOLOGY VALIDATION ===\n",
      "Built mapping for 14 stop names\n",
      "✅ Rydsvägens ändhållplats: Valid mapping\n",
      "✅ Björnkärrsskolan: Valid mapping\n",
      "✅ Alsättersgatan: Valid mapping\n",
      "✅ Hangaren: Valid mapping\n",
      "✅ Mårdtorpsgatan: Valid mapping\n",
      "✅ Linköpings resecentrum: Valid mapping\n",
      "✅ Ryd centrum: Valid mapping\n",
      "✅ Kungsgatan: Valid mapping\n",
      "✅ Stora Torget: Valid mapping\n",
      "✅ Solhaga: Valid mapping\n",
      "✅ Barnhemsgatan: Valid mapping\n",
      "✅ Parkgatan: Valid mapping\n",
      "✅ Gamla Linköping: Valid mapping\n",
      "✅ Vallaplan: Valid mapping\n",
      "Validation complete: 0 stop_id-direction violations detected (detailed logging deferred)\n",
      "\n",
      "=== UNIFIED TRIP CLASSIFICATION WITH GAP DETECTION ===\n",
      "Direction 1: canonical pattern 14 stops (from 7790 trips)\n",
      "Direction 0: canonical pattern 14 stops (from 8164 trips)\n",
      "Complete: 18456 trips, 2 stop-level violations affecting 32331 records\n",
      "Additional info: 32453 total non-full trip records\n",
      "\n",
      "=== FINALIZING TOPOLOGY VIOLATIONS LOG ===\n",
      "Finalized 0 detailed topology violations with trip_type granularity\n",
      "\n",
      "=== DETECTING REGULATORY STOPS ===\n",
      "Found 42 regulatory combinations\n",
      "Regulatory violations created: 0\n",
      "Regulatory analysis complete: 42 combinations, 0 violations\n",
      "\n",
      "=== CALCULATING TRAVEL TIMES AND DELAYS ===\n",
      "  Using stop-level pattern violations for travel time validation\n",
      "Calculation complete: 234866/255809 valid segments (91.8%)\n",
      "  Using stop-level pattern violations for precise validation\n",
      "\n",
      "=== CREATING MASTER INDEXER ===\n",
      "Processing 140 unique combinations\n",
      "Master indexer created: 140 combinations\n",
      "  - Flagged: 4 (2.9%)\n",
      "  - High severity: 0\n",
      "  - Medium severity: 4\n",
      "  - Low severity: 0\n",
      "\n",
      "=== CREATING NEW NAVIGATION MAPS ===\n",
      "Using topology validation results for stop mapping\n",
      "Processing 382 unique data combinations\n",
      "Navigation maps created:\n",
      "  - 26 stops in combinations\n",
      "  - 1 routes\n",
      "  - 14 stop names mapped\n",
      "\n",
      "=== EXPORTING ALL DATA WITH GLOBAL MERGING ===\n",
      "Using global output folder: transit_analysis_global\n",
      "✅ master_indexer.json: Route cleanup + merge (452 → 592, +140 new)\n",
      "✅ log_pattern_violations.json: Route cleanup + merge (11 → 13, +2 new)\n",
      "✅ log_trip_types.json: Route cleanup + merge (452 → 592, +140 new)\n",
      "✅ log_regulatory_stops.json: Route cleanup + merge (87 → 129, +42 new)\n",
      "✅ global_stop_to_combinations.json: Global navigation accumulated (multi-route)\n",
      "✅ global_route_to_combinations.json: Global navigation accumulated (multi-route)\n",
      "✅ global_stop_name_to_stop_ids.json: Global navigation accumulated (multi-route)\n",
      "✅ global_route_short_name_to_info.json: Global navigation accumulated (multi-route)\n",
      "✅ Global summary updated: transit_analysis_global\\global_summary.json\n",
      "\n",
      "🌍 GLOBAL ANALYSIS STATUS:\n",
      "  - Routes analyzed: 3\n",
      "  - Stop names: 51\n",
      "  - Total combinations: 592\n",
      "  - Issues detected: Yes\n",
      "  - Global files: transit_analysis_global\n",
      "  - Use master_indexer.json for detailed violation analysis\n",
      "processing raw data for route 204\n",
      "Removing 0 duplicates.\n",
      "\n",
      "=== STOP TOPOLOGY VALIDATION ===\n",
      "Built mapping for 18 stop names\n",
      "✅ Landbogatan: Valid mapping\n",
      "✅ Änggårdsskolan: Valid mapping\n",
      "✅ Boställsgatan: Valid mapping\n",
      "✅ Slestadsskolan: Valid mapping\n",
      "✅ Rättaregatan: Valid mapping\n",
      "✅ Isberget: Valid mapping\n",
      "✅ Teknikringen: Valid mapping\n",
      "✅ Nobeltorget: Valid mapping\n",
      "✅ Linköpings resecentrum: Valid mapping\n",
      "✅ Kungsgatan: Valid mapping\n",
      "✅ Djurgården: Valid mapping\n",
      "✅ US Norra entrén: Valid mapping\n",
      "✅ US Södra entrén: Valid mapping\n",
      "✅ Trädgårdstorget: Valid mapping\n",
      "✅ Tinnerbäcksbadet: Valid mapping\n",
      "✅ Infanterivägen: Valid mapping\n",
      "✅ Vallastadens skola: Valid mapping\n",
      "✅ Garnisonen: Valid mapping\n",
      "Validation complete: 0 stop_id-direction violations detected (detailed logging deferred)\n",
      "\n",
      "=== UNIFIED TRIP CLASSIFICATION WITH GAP DETECTION ===\n",
      "Direction 1: canonical pattern 17 stops (from 5533 trips)\n",
      "Direction 0: canonical pattern 17 stops (from 5662 trips)\n",
      "Complete: 14991 trips, 4 stop-level violations affecting 62349 records\n",
      "Additional info: 62457 total non-full trip records\n",
      "\n",
      "=== FINALIZING TOPOLOGY VIOLATIONS LOG ===\n",
      "Finalized 0 detailed topology violations with trip_type granularity\n",
      "\n",
      "=== DETECTING REGULATORY STOPS ===\n",
      "Found 39 regulatory combinations\n",
      "Regulatory violations created: 0\n",
      "Regulatory analysis complete: 39 combinations, 0 violations\n",
      "\n",
      "=== CALCULATING TRAVEL TIMES AND DELAYS ===\n",
      "  Using stop-level pattern violations for travel time validation\n",
      "Calculation complete: 233995/252772 valid segments (92.6%)\n",
      "  Using stop-level pattern violations for precise validation\n",
      "\n",
      "=== CREATING MASTER INDEXER ===\n",
      "Processing 165 unique combinations\n",
      "Master indexer created: 165 combinations\n",
      "  - Flagged: 66 (40.0%)\n",
      "  - High severity: 0\n",
      "  - Medium severity: 66\n",
      "  - Low severity: 0\n",
      "\n",
      "=== CREATING NEW NAVIGATION MAPS ===\n",
      "Using topology validation results for stop mapping\n",
      "Processing 589 unique data combinations\n",
      "Navigation maps created:\n",
      "  - 35 stops in combinations\n",
      "  - 1 routes\n",
      "  - 18 stop names mapped\n",
      "\n",
      "=== EXPORTING ALL DATA WITH GLOBAL MERGING ===\n",
      "Using global output folder: transit_analysis_global\n",
      "✅ master_indexer.json: Route cleanup + merge (592 → 757, +165 new)\n",
      "✅ log_pattern_violations.json: Route cleanup + merge (13 → 17, +4 new)\n",
      "✅ log_trip_types.json: Route cleanup + merge (592 → 757, +165 new)\n",
      "✅ log_regulatory_stops.json: Route cleanup + merge (129 → 168, +39 new)\n",
      "✅ global_stop_to_combinations.json: Global navigation accumulated (multi-route)\n",
      "✅ global_route_to_combinations.json: Global navigation accumulated (multi-route)\n",
      "✅ global_stop_name_to_stop_ids.json: Global navigation accumulated (multi-route)\n",
      "✅ global_route_short_name_to_info.json: Global navigation accumulated (multi-route)\n",
      "✅ Global summary updated: transit_analysis_global\\global_summary.json\n",
      "\n",
      "🌍 GLOBAL ANALYSIS STATUS:\n",
      "  - Routes analyzed: 4\n",
      "  - Stop names: 69\n",
      "  - Total combinations: 757\n",
      "  - Issues detected: Yes\n",
      "  - Global files: transit_analysis_global\n",
      "  - Use master_indexer.json for detailed violation analysis\n",
      "processing raw data for route 205\n",
      "Removing 0 duplicates.\n",
      "\n",
      "=== STOP TOPOLOGY VALIDATION ===\n",
      "Built mapping for 29 stop names\n",
      "✅ Räknestickan: Valid mapping\n",
      "✅ Ekholmsskolan: Valid mapping\n",
      "✅ Vårdcentralen Ekholmen: Valid mapping\n",
      "✅ Ekholmens centrum: Valid mapping\n",
      "✅ Järdalavägen 50: Valid mapping\n",
      "✅ Risbrinksgatan: Valid mapping\n",
      "✅ Folkungavallen: Valid mapping\n",
      "✅ Ekhaga: Valid mapping\n",
      "✅ Kristinebergsgatan: Valid mapping\n",
      "✅ Jakobsdal: Valid mapping\n",
      "✅ Braskens bro: Valid mapping\n",
      "✅ Skäggetorp centrum: Valid mapping\n",
      "✅ Mellangården: Valid mapping\n",
      "✅ Rusthållaregården: Valid mapping\n",
      "✅ Tinnerbäcksbadet: Valid mapping\n",
      "✅ Sörgårdsgatan: Valid mapping\n",
      "✅ Trädgårdstorget: Valid mapping\n",
      "✅ Stiglötsgatan: Valid mapping\n",
      "✅ Kungsgatan: Valid mapping\n",
      "✅ Fogdegatan: Valid mapping\n",
      "✅ Skrivaregatan: Valid mapping\n",
      "✅ Skattegården: Valid mapping\n",
      "✅ Linköpings resecentrum: Valid mapping\n",
      "✅ IKEA: Valid mapping\n",
      "✅ Norra Tornby: Valid mapping\n",
      "✅ Tornet: Valid mapping\n",
      "✅ Tornby Park: Valid mapping\n",
      "✅ Fröstorpsgatan: Valid mapping\n",
      "✅ Bergdalsgatan: Valid mapping\n",
      "Validation complete: 0 stop_id-direction violations detected (detailed logging deferred)\n",
      "\n",
      "=== UNIFIED TRIP CLASSIFICATION WITH GAP DETECTION ===\n",
      "Direction 1: canonical pattern 27 stops (from 8108 trips)\n",
      "Direction 0: canonical pattern 28 stops (from 8190 trips)\n",
      "Complete: 18886 trips, 6 stop-level violations affecting 67406 records\n",
      "Additional info: 68096 total non-full trip records\n",
      "\n",
      "=== FINALIZING TOPOLOGY VIOLATIONS LOG ===\n",
      "Finalized 0 detailed topology violations with trip_type granularity\n",
      "\n",
      "=== DETECTING REGULATORY STOPS ===\n",
      "Found 119 regulatory combinations\n",
      "Regulatory violations created: 0\n",
      "Regulatory analysis complete: 119 combinations, 0 violations\n",
      "\n",
      "=== CALCULATING TRAVEL TIMES AND DELAYS ===\n",
      "  Using stop-level pattern violations for travel time validation\n",
      "Calculation complete: 494899/516332 valid segments (95.8%)\n",
      "  Using stop-level pattern violations for precise validation\n",
      "\n",
      "=== CREATING MASTER INDEXER ===\n",
      "Processing 574 unique combinations\n",
      "Master indexer created: 574 combinations\n",
      "  - Flagged: 74 (12.9%)\n",
      "  - High severity: 0\n",
      "  - Medium severity: 74\n",
      "  - Low severity: 0\n",
      "\n",
      "=== CREATING NEW NAVIGATION MAPS ===\n",
      "Using topology validation results for stop mapping\n",
      "Processing 1254 unique data combinations\n",
      "Navigation maps created:\n",
      "  - 54 stops in combinations\n",
      "  - 1 routes\n",
      "  - 29 stop names mapped\n",
      "\n",
      "=== EXPORTING ALL DATA WITH GLOBAL MERGING ===\n",
      "Using global output folder: transit_analysis_global\n",
      "✅ master_indexer.json: Route cleanup + merge (757 → 1331, +574 new)\n",
      "✅ log_pattern_violations.json: Route cleanup + merge (17 → 23, +6 new)\n",
      "✅ log_trip_types.json: Route cleanup + merge (757 → 1331, +574 new)\n",
      "✅ log_regulatory_stops.json: Route cleanup + merge (168 → 287, +119 new)\n",
      "✅ global_stop_to_combinations.json: Global navigation accumulated (multi-route)\n",
      "✅ global_route_to_combinations.json: Global navigation accumulated (multi-route)\n",
      "✅ global_stop_name_to_stop_ids.json: Global navigation accumulated (multi-route)\n",
      "✅ global_route_short_name_to_info.json: Global navigation accumulated (multi-route)\n",
      "✅ Global summary updated: transit_analysis_global\\global_summary.json\n",
      "\n",
      "🌍 GLOBAL ANALYSIS STATUS:\n",
      "  - Routes analyzed: 5\n",
      "  - Stop names: 98\n",
      "  - Total combinations: 1331\n",
      "  - Issues detected: Yes\n",
      "  - Global files: transit_analysis_global\n",
      "  - Use master_indexer.json for detailed violation analysis\n",
      "processing raw data for route 206\n",
      "Removing 0 duplicates.\n",
      "\n",
      "=== STOP TOPOLOGY VALIDATION ===\n",
      "Built mapping for 17 stop names\n",
      "✅ Fårullsvägen: Valid mapping\n",
      "✅ Vinkännaren: Valid mapping\n",
      "✅ Tokarps skola: Valid mapping\n",
      "✅ Fårhagsvägen: Valid mapping\n",
      "✅ Fårbetesvägen: Valid mapping\n",
      "✅ Kärnabrunnsgatan: Valid mapping\n",
      "✅ Kärna skola: Valid mapping\n",
      "✅ Kärna kors: Valid mapping\n",
      "✅ Carl Cederströms gata: Valid mapping\n",
      "✅ Flygvapenmuseum: Valid mapping\n",
      "✅ Linköpings resecentrum: Valid mapping\n",
      "✅ Stora Torget: Valid mapping\n",
      "✅ Parkgatan: Valid mapping\n",
      "✅ Vallaplan: Valid mapping\n",
      "✅ Gamla Linköping: Valid mapping\n",
      "✅ Kungsgatan: Valid mapping\n",
      "✅ Barnhemsgatan: Valid mapping\n",
      "Validation complete: 0 stop_id-direction violations detected (detailed logging deferred)\n",
      "\n",
      "=== UNIFIED TRIP CLASSIFICATION WITH GAP DETECTION ===\n",
      "Direction 1: canonical pattern 17 stops (from 5482 trips)\n",
      "Direction 0: canonical pattern 17 stops (from 5654 trips)\n",
      "Complete: 14904 trips, 5 stop-level violations affecting 60088 records\n",
      "Additional info: 60191 total non-full trip records\n",
      "\n",
      "=== FINALIZING TOPOLOGY VIOLATIONS LOG ===\n",
      "Finalized 0 detailed topology violations with trip_type granularity\n",
      "\n",
      "=== DETECTING REGULATORY STOPS ===\n",
      "Found 37 regulatory combinations\n",
      "Regulatory violations created: 0\n",
      "Regulatory analysis complete: 37 combinations, 0 violations\n",
      "\n",
      "=== CALCULATING TRAVEL TIMES AND DELAYS ===\n",
      "  Using stop-level pattern violations for travel time validation\n",
      "Calculation complete: 227089/249503 valid segments (91.0%)\n",
      "  Using stop-level pattern violations for precise validation\n",
      "\n",
      "=== CREATING MASTER INDEXER ===\n",
      "Processing 158 unique combinations\n",
      "Master indexer created: 158 combinations\n",
      "  - Flagged: 43 (27.2%)\n",
      "  - High severity: 0\n",
      "  - Medium severity: 43\n",
      "  - Low severity: 0\n",
      "\n",
      "=== CREATING NEW NAVIGATION MAPS ===\n",
      "Using topology validation results for stop mapping\n",
      "Processing 458 unique data combinations\n",
      "Navigation maps created:\n",
      "  - 33 stops in combinations\n",
      "  - 1 routes\n",
      "  - 17 stop names mapped\n",
      "\n",
      "=== EXPORTING ALL DATA WITH GLOBAL MERGING ===\n",
      "Using global output folder: transit_analysis_global\n",
      "✅ master_indexer.json: Route cleanup + merge (1331 → 1489, +158 new)\n",
      "✅ log_pattern_violations.json: Route cleanup + merge (23 → 28, +5 new)\n",
      "✅ log_trip_types.json: Route cleanup + merge (1331 → 1489, +158 new)\n",
      "✅ log_regulatory_stops.json: Route cleanup + merge (287 → 324, +37 new)\n",
      "✅ global_stop_to_combinations.json: Global navigation accumulated (multi-route)\n",
      "✅ global_route_to_combinations.json: Global navigation accumulated (multi-route)\n",
      "✅ global_stop_name_to_stop_ids.json: Global navigation accumulated (multi-route)\n",
      "✅ global_route_short_name_to_info.json: Global navigation accumulated (multi-route)\n",
      "✅ Global summary updated: transit_analysis_global\\global_summary.json\n",
      "\n",
      "🌍 GLOBAL ANALYSIS STATUS:\n",
      "  - Routes analyzed: 6\n",
      "  - Stop names: 115\n",
      "  - Total combinations: 1489\n",
      "  - Issues detected: Yes\n",
      "  - Global files: transit_analysis_global\n",
      "  - Use master_indexer.json for detailed violation analysis\n",
      "processing raw data for route 210\n",
      "Removing 0 duplicates.\n",
      "\n",
      "=== STOP TOPOLOGY VALIDATION ===\n",
      "Built mapping for 31 stop names\n",
      "✅ Kråkbärsvägen: Valid mapping\n",
      "✅ Vist skola: Valid mapping\n",
      "✅ Norrbergavägen: Valid mapping\n",
      "✅ Klampenborg: Valid mapping\n",
      "✅ Sturefors centrum: Valid mapping\n",
      "✅ Roxtuna: Valid mapping\n",
      "✅ Harvestad: Valid mapping\n",
      "✅ Udden: Valid mapping\n",
      "✅ Södra Ullstämma: Valid mapping\n",
      "✅ Östernäs: Valid mapping\n",
      "✅ Kungsgatan: Valid mapping\n",
      "✅ Hallonvägen: Valid mapping\n",
      "✅ Ekholmen Brokindsleden: Valid mapping\n",
      "✅ Jakobsdal: Valid mapping\n",
      "✅ Tinnerbäcksbadet: Valid mapping\n",
      "✅ Trädgårdstorget: Valid mapping\n",
      "✅ Kristinebergsgatan: Valid mapping\n",
      "✅ Folkungavallen: Valid mapping\n",
      "✅ Braskens bro: Valid mapping\n",
      "✅ Risbrinksgatan: Valid mapping\n",
      "✅ Bryggvägen: Valid mapping\n",
      "✅ Stensättersvägen: Valid mapping\n",
      "✅ Näsby allé: Valid mapping\n",
      "✅ Gärstad: Valid mapping\n",
      "✅ Åby: Valid mapping\n",
      "✅ Runstensgatan: Valid mapping\n",
      "✅ Sigbjörnsgatan: Valid mapping\n",
      "✅ Stångebro: Valid mapping\n",
      "✅ Anders Ljungstedts gy.: Valid mapping\n",
      "✅ Linköpings resecentrum: Valid mapping\n",
      "✅ Bergdalsgatan: Valid mapping\n",
      "Validation complete: 0 stop_id-direction violations detected (detailed logging deferred)\n",
      "\n",
      "=== UNIFIED TRIP CLASSIFICATION WITH GAP DETECTION ===\n",
      "Direction 1: canonical pattern 29 stops (from 4016 trips)\n",
      "Direction 0: canonical pattern 29 stops (from 3985 trips)\n",
      "Complete: 9890 trips, 4 stop-level violations affecting 35757 records\n",
      "Additional info: 44852 total non-full trip records\n",
      "\n",
      "=== FINALIZING TOPOLOGY VIOLATIONS LOG ===\n",
      "Finalized 0 detailed topology violations with trip_type granularity\n",
      "\n",
      "=== DETECTING REGULATORY STOPS ===\n",
      "Found 88 regulatory combinations\n",
      "  - 1 with anomalies\n",
      "Regulatory violations created: 1\n",
      "Regulatory analysis complete: 88 combinations, 1 violations\n",
      "\n",
      "=== CALCULATING TRAVEL TIMES AND DELAYS ===\n",
      "  Using stop-level pattern violations for travel time validation\n",
      "Calculation complete: 265690/276881 valid segments (96.0%)\n",
      "  Using stop-level pattern violations for precise validation\n",
      "\n",
      "=== CREATING MASTER INDEXER ===\n",
      "Processing 391 unique combinations\n",
      "Master indexer created: 391 combinations\n",
      "  - Flagged: 36 (9.2%)\n",
      "  - High severity: 0\n",
      "  - Medium severity: 35\n",
      "  - Low severity: 1\n",
      "\n",
      "=== CREATING NEW NAVIGATION MAPS ===\n",
      "Using topology validation results for stop mapping\n",
      "Processing 864 unique data combinations\n",
      "Navigation maps created:\n",
      "  - 57 stops in combinations\n",
      "  - 1 routes\n",
      "  - 31 stop names mapped\n",
      "\n",
      "=== EXPORTING ALL DATA WITH GLOBAL MERGING ===\n",
      "Using global output folder: transit_analysis_global\n",
      "✅ master_indexer.json: Route cleanup + merge (1489 → 1880, +391 new)\n",
      "✅ log_pattern_violations.json: Route cleanup + merge (28 → 32, +4 new)\n",
      "✅ log_regulatory_violations.json: Created new file (1 entries)\n",
      "✅ log_trip_types.json: Route cleanup + merge (1489 → 1880, +391 new)\n",
      "✅ log_regulatory_stops.json: Route cleanup + merge (324 → 412, +88 new)\n",
      "✅ global_stop_to_combinations.json: Global navigation accumulated (multi-route)\n",
      "✅ global_route_to_combinations.json: Global navigation accumulated (multi-route)\n",
      "✅ global_stop_name_to_stop_ids.json: Global navigation accumulated (multi-route)\n",
      "✅ global_route_short_name_to_info.json: Global navigation accumulated (multi-route)\n",
      "✅ Global summary updated: transit_analysis_global\\global_summary.json\n",
      "\n",
      "🌍 GLOBAL ANALYSIS STATUS:\n",
      "  - Routes analyzed: 7\n",
      "  - Stop names: 146\n",
      "  - Total combinations: 1880\n",
      "  - Issues detected: Yes\n",
      "  - Global files: transit_analysis_global\n",
      "  - Use master_indexer.json for detailed violation analysis\n",
      "processing raw data for route 211\n",
      "Removing 0 duplicates.\n",
      "\n",
      "=== STOP TOPOLOGY VALIDATION ===\n",
      "Built mapping for 13 stop names\n",
      "✅ Linköpings resecentrum: Valid mapping\n",
      "✅ Kungsgatan: Valid mapping\n",
      "✅ Ödegårdsgatan 11: Valid mapping\n",
      "✅ Övre Johannelund: Valid mapping\n",
      "✅ Tenndosan: Valid mapping\n",
      "✅ Drottningtorget: Valid mapping\n",
      "✅ Loftgatan: Valid mapping\n",
      "✅ Tegelbruksgatan: Valid mapping\n",
      "✅ Sandgårdsgatan: Valid mapping\n",
      "✅ Ödegårdsgatan 24: Valid mapping\n",
      "✅ Johannelunds centrum: Valid mapping\n",
      "✅ Stationsgatan: Valid mapping\n",
      "✅ Trädgårdstorget: Valid mapping\n",
      "Validation complete: 0 stop_id-direction violations detected (detailed logging deferred)\n",
      "\n",
      "=== UNIFIED TRIP CLASSIFICATION WITH GAP DETECTION ===\n",
      "Direction 1: canonical pattern 13 stops (from 5225 trips)\n",
      "Direction 0: canonical pattern 13 stops (from 5386 trips)\n",
      "Complete: 12268 trips, 2 stop-level violations affecting 19836 records\n",
      "Additional info: 19869 total non-full trip records\n",
      "\n",
      "=== FINALIZING TOPOLOGY VIOLATIONS LOG ===\n",
      "Finalized 0 detailed topology violations with trip_type granularity\n",
      "\n",
      "=== DETECTING REGULATORY STOPS ===\n",
      "Found 21 regulatory combinations\n",
      "Regulatory violations created: 0\n",
      "Regulatory analysis complete: 21 combinations, 0 violations\n",
      "\n",
      "=== CALCULATING TRAVEL TIMES AND DELAYS ===\n",
      "  Using stop-level pattern violations for travel time validation\n",
      "Calculation complete: 143891/157812 valid segments (91.2%)\n",
      "  Using stop-level pattern violations for precise validation\n",
      "\n",
      "=== CREATING MASTER INDEXER ===\n",
      "Processing 83 unique combinations\n",
      "Master indexer created: 83 combinations\n",
      "  - Flagged: 24 (28.9%)\n",
      "  - High severity: 0\n",
      "  - Medium severity: 24\n",
      "  - Low severity: 0\n",
      "\n",
      "=== CREATING NEW NAVIGATION MAPS ===\n",
      "Using topology validation results for stop mapping\n",
      "Processing 283 unique data combinations\n",
      "Navigation maps created:\n",
      "  - 24 stops in combinations\n",
      "  - 1 routes\n",
      "  - 13 stop names mapped\n",
      "\n",
      "=== EXPORTING ALL DATA WITH GLOBAL MERGING ===\n",
      "Using global output folder: transit_analysis_global\n",
      "✅ master_indexer.json: Route cleanup + merge (1880 → 1963, +83 new)\n",
      "✅ log_pattern_violations.json: Route cleanup + merge (32 → 34, +2 new)\n",
      "✅ log_trip_types.json: Route cleanup + merge (1880 → 1963, +83 new)\n",
      "✅ log_regulatory_stops.json: Route cleanup + merge (412 → 433, +21 new)\n",
      "✅ global_stop_to_combinations.json: Global navigation accumulated (multi-route)\n",
      "✅ global_route_to_combinations.json: Global navigation accumulated (multi-route)\n",
      "✅ global_stop_name_to_stop_ids.json: Global navigation accumulated (multi-route)\n",
      "✅ global_route_short_name_to_info.json: Global navigation accumulated (multi-route)\n",
      "✅ Global summary updated: transit_analysis_global\\global_summary.json\n",
      "\n",
      "🌍 GLOBAL ANALYSIS STATUS:\n",
      "  - Routes analyzed: 8\n",
      "  - Stop names: 159\n",
      "  - Total combinations: 1963\n",
      "  - Issues detected: Yes\n",
      "  - Global files: transit_analysis_global\n",
      "  - Use master_indexer.json for detailed violation analysis\n",
      "processing raw data for route 212\n",
      "Removing 0 duplicates.\n",
      "\n",
      "=== STOP TOPOLOGY VALIDATION ===\n",
      "Built mapping for 17 stop names\n",
      "✅ Linköpings resecentrum: Valid mapping\n",
      "✅ Klostergatan: Valid mapping\n",
      "✅ Konsert & Kongress: Valid mapping\n",
      "✅ Länsstyrelsen: Valid mapping\n",
      "✅ Parkgatan: Valid mapping\n",
      "✅ Vallaplan: Valid mapping\n",
      "✅ Gamla Linköping: Valid mapping\n",
      "✅ Slestadsskolan: Valid mapping\n",
      "✅ Rättaregatan: Valid mapping\n",
      "✅ Boställsgatan: Valid mapping\n",
      "✅ Bygärdesgatan: Valid mapping\n",
      "✅ Mjärdevi Center: Valid mapping\n",
      "✅ Forskningsbyn: Valid mapping\n",
      "✅ Isberget: Valid mapping\n",
      "✅ Universitetet: Valid mapping\n",
      "✅ Mäster Mattias väg: Valid mapping\n",
      "✅ Änggårdsskolan: Valid mapping\n",
      "Validation complete: 0 stop_id-direction violations detected (detailed logging deferred)\n",
      "\n",
      "=== UNIFIED TRIP CLASSIFICATION WITH GAP DETECTION ===\n",
      "Direction 1: canonical pattern 17 stops (from 3996 trips)\n",
      "Direction 0: canonical pattern 17 stops (from 3839 trips)\n",
      "Complete: 7843 trips, 0 stop-level violations affecting 0 records\n",
      "Additional info: 101 total non-full trip records\n",
      "\n",
      "=== FINALIZING TOPOLOGY VIOLATIONS LOG ===\n",
      "Finalized 0 detailed topology violations with trip_type granularity\n",
      "\n",
      "=== DETECTING REGULATORY STOPS ===\n",
      "Found 28 regulatory combinations\n",
      "Regulatory violations created: 0\n",
      "Regulatory analysis complete: 28 combinations, 0 violations\n",
      "\n",
      "=== CALCULATING TRAVEL TIMES AND DELAYS ===\n",
      "  Using stop-level pattern violations for travel time validation\n",
      "Calculation complete: 125453/133296 valid segments (94.1%)\n",
      "  Using stop-level pattern violations for precise validation\n",
      "\n",
      "=== CREATING MASTER INDEXER ===\n",
      "Processing 121 unique combinations\n",
      "Master indexer created: 121 combinations\n",
      "  - Flagged: 0 (0.0%)\n",
      "  - High severity: 0\n",
      "  - Medium severity: 0\n",
      "  - Low severity: 0\n",
      "\n",
      "=== CREATING NEW NAVIGATION MAPS ===\n",
      "Using topology validation results for stop mapping\n",
      "Processing 271 unique data combinations\n",
      "Navigation maps created:\n",
      "  - 32 stops in combinations\n",
      "  - 1 routes\n",
      "  - 17 stop names mapped\n",
      "\n",
      "=== EXPORTING ALL DATA WITH GLOBAL MERGING ===\n",
      "Using global output folder: transit_analysis_global\n",
      "✅ master_indexer.json: Route cleanup + merge (1963 → 2084, +121 new)\n",
      "✅ log_trip_types.json: Route cleanup + merge (1963 → 2084, +121 new)\n",
      "✅ log_regulatory_stops.json: Route cleanup + merge (433 → 461, +28 new)\n",
      "✅ global_stop_to_combinations.json: Global navigation accumulated (multi-route)\n",
      "✅ global_route_to_combinations.json: Global navigation accumulated (multi-route)\n",
      "✅ global_stop_name_to_stop_ids.json: Global navigation accumulated (multi-route)\n",
      "✅ global_route_short_name_to_info.json: Global navigation accumulated (multi-route)\n",
      "✅ Global summary updated: transit_analysis_global\\global_summary.json\n",
      "\n",
      "🌍 GLOBAL ANALYSIS STATUS:\n",
      "  - Routes analyzed: 9\n",
      "  - Stop names: 176\n",
      "  - Total combinations: 2084\n",
      "  - Issues detected: Yes\n",
      "  - Global files: transit_analysis_global\n",
      "  - Use master_indexer.json for detailed violation analysis\n",
      "processing raw data for route 213\n",
      "Removing 0 duplicates.\n",
      "\n",
      "=== STOP TOPOLOGY VALIDATION ===\n",
      "Built mapping for 28 stop names\n",
      "✅ Munkhagsgatan 74: Valid mapping\n",
      "✅ Risbrinksgatan: Valid mapping\n",
      "✅ Kristinebergsgatan: Valid mapping\n",
      "✅ Spångerum: Valid mapping\n",
      "✅ Ödegårdsgatan 24: Valid mapping\n",
      "✅ Tinnerbäcksbadet: Valid mapping\n",
      "✅ Johannelunds centrum: Valid mapping\n",
      "✅ Nedre Johannelund: Valid mapping\n",
      "✅ Ödegårdsgatan 11: Valid mapping\n",
      "✅ Folkungavallen: Valid mapping\n",
      "✅ Trädgårdstorget: Valid mapping\n",
      "✅ Kungsgatan: Valid mapping\n",
      "✅ Linköpings resecentrum: Valid mapping\n",
      "✅ Anders Ljungstedts gy.: Valid mapping\n",
      "✅ Linköping Arena: Valid mapping\n",
      "✅ Tallboda centrum: Valid mapping\n",
      "✅ Harstenagatan: Valid mapping\n",
      "✅ Tallboda skola: Valid mapping\n",
      "✅ Idögatan: Valid mapping\n",
      "✅ Gullringsvägen: Valid mapping\n",
      "✅ Masugnen: Valid mapping\n",
      "✅ Fågelögatan: Valid mapping\n",
      "✅ Sidvindsvägen: Valid mapping\n",
      "✅ Fönvindsvägen östra: Valid mapping\n",
      "✅ Staby bro: Valid mapping\n",
      "✅ Fönvindsvägen västra: Valid mapping\n",
      "✅ Bergdalsgatan: Valid mapping\n",
      "✅ Gränsliden: Valid mapping\n",
      "Validation complete: 0 stop_id-direction violations detected (detailed logging deferred)\n",
      "\n",
      "=== UNIFIED TRIP CLASSIFICATION WITH GAP DETECTION ===\n",
      "Direction 1: canonical pattern 25 stops (from 4622 trips)\n",
      "Direction 0: canonical pattern 26 stops (from 4683 trips)\n",
      "Complete: 12457 trips, 7 stop-level violations affecting 76655 records\n",
      "Additional info: 76974 total non-full trip records\n",
      "\n",
      "=== FINALIZING TOPOLOGY VIOLATIONS LOG ===\n",
      "Finalized 0 detailed topology violations with trip_type granularity\n",
      "\n",
      "=== DETECTING REGULATORY STOPS ===\n",
      "Found 81 regulatory combinations\n",
      "Regulatory violations created: 0\n",
      "Regulatory analysis complete: 81 combinations, 0 violations\n",
      "\n",
      "=== CALCULATING TRAVEL TIMES AND DELAYS ===\n",
      "  Using stop-level pattern violations for travel time validation\n",
      "Calculation complete: 295569/314282 valid segments (94.0%)\n",
      "  Using stop-level pattern violations for precise validation\n",
      "\n",
      "=== CREATING MASTER INDEXER ===\n",
      "Processing 366 unique combinations\n",
      "Master indexer created: 366 combinations\n",
      "  - Flagged: 90 (24.6%)\n",
      "  - High severity: 0\n",
      "  - Medium severity: 90\n",
      "  - Low severity: 0\n",
      "\n",
      "=== CREATING NEW NAVIGATION MAPS ===\n",
      "Using topology validation results for stop mapping\n",
      "Processing 844 unique data combinations\n",
      "Navigation maps created:\n",
      "  - 54 stops in combinations\n",
      "  - 1 routes\n",
      "  - 28 stop names mapped\n",
      "\n",
      "=== EXPORTING ALL DATA WITH GLOBAL MERGING ===\n",
      "Using global output folder: transit_analysis_global\n",
      "✅ master_indexer.json: Route cleanup + merge (2084 → 2450, +366 new)\n",
      "✅ log_pattern_violations.json: Route cleanup + merge (34 → 41, +7 new)\n",
      "✅ log_trip_types.json: Route cleanup + merge (2084 → 2450, +366 new)\n",
      "✅ log_regulatory_stops.json: Route cleanup + merge (461 → 542, +81 new)\n",
      "✅ global_stop_to_combinations.json: Global navigation accumulated (multi-route)\n",
      "✅ global_route_to_combinations.json: Global navigation accumulated (multi-route)\n",
      "✅ global_stop_name_to_stop_ids.json: Global navigation accumulated (multi-route)\n",
      "✅ global_route_short_name_to_info.json: Global navigation accumulated (multi-route)\n",
      "✅ Global summary updated: transit_analysis_global\\global_summary.json\n",
      "\n",
      "🌍 GLOBAL ANALYSIS STATUS:\n",
      "  - Routes analyzed: 10\n",
      "  - Stop names: 204\n",
      "  - Total combinations: 2450\n",
      "  - Issues detected: Yes\n",
      "  - Global files: transit_analysis_global\n",
      "  - Use master_indexer.json for detailed violation analysis\n",
      "processing raw data for route 214\n",
      "Removing 0 duplicates.\n",
      "\n",
      "=== STOP TOPOLOGY VALIDATION ===\n",
      "Built mapping for 30 stop names\n",
      "✅ Aspnäset: Valid mapping\n",
      "✅ Lövsbergsvägen: Valid mapping\n",
      "✅ Ekdalsvägen: Valid mapping\n",
      "✅ Vårdsbergsvägen: Valid mapping\n",
      "✅ Strandängsvägen: Valid mapping\n",
      "✅ Aspeliden: Valid mapping\n",
      "✅ Kvinnebystigen: Valid mapping\n",
      "✅ Ektunavägen: Valid mapping\n",
      "✅ Ekholmens centrum: Valid mapping\n",
      "✅ Järdalavägen 50: Valid mapping\n",
      "✅ Ekhaga: Valid mapping\n",
      "✅ Majelden: Valid mapping\n",
      "✅ Kryddvägen: Valid mapping\n",
      "✅ Ridhusgatan: Valid mapping\n",
      "✅ Resedan: Valid mapping\n",
      "✅ Berga Söderleden: Valid mapping\n",
      "✅ Kattfotstigen: Valid mapping\n",
      "✅ Folkungavallen: Valid mapping\n",
      "✅ Tornhagen: Valid mapping\n",
      "✅ Gamla Linköping: Valid mapping\n",
      "✅ Tinnerbäcksbadet: Valid mapping\n",
      "✅ Lektorshagen: Valid mapping\n",
      "✅ Trädgårdstorget: Valid mapping\n",
      "✅ Abisko centrum: Valid mapping\n",
      "✅ Timmermansplatsen: Valid mapping\n",
      "✅ Gärdesgatan: Valid mapping\n",
      "✅ Kungsgatan: Valid mapping\n",
      "✅ Linköpings resecentrum: Valid mapping\n",
      "✅ Bergdalsgatan: Valid mapping\n",
      "✅ Åleryd: Valid mapping\n",
      "Validation complete: 0 stop_id-direction violations detected (detailed logging deferred)\n",
      "\n",
      "=== UNIFIED TRIP CLASSIFICATION WITH GAP DETECTION ===\n",
      "Direction 1: canonical pattern 28 stops (from 4057 trips)\n",
      "Direction 0: canonical pattern 28 stops (from 4112 trips)\n",
      "Complete: 9541 trips, 5 stop-level violations affecting 34638 records\n",
      "Additional info: 36439 total non-full trip records\n",
      "\n",
      "=== FINALIZING TOPOLOGY VIOLATIONS LOG ===\n",
      "Finalized 0 detailed topology violations with trip_type granularity\n",
      "\n",
      "=== DETECTING REGULATORY STOPS ===\n",
      "Found 85 regulatory combinations\n",
      "Regulatory violations created: 0\n",
      "Regulatory analysis complete: 85 combinations, 0 violations\n",
      "\n",
      "=== CALCULATING TRAVEL TIMES AND DELAYS ===\n",
      "  Using stop-level pattern violations for travel time validation\n",
      "Calculation complete: 254343/265171 valid segments (95.9%)\n",
      "  Using stop-level pattern violations for precise validation\n",
      "\n",
      "=== CREATING MASTER INDEXER ===\n",
      "Processing 404 unique combinations\n",
      "Master indexer created: 404 combinations\n",
      "  - Flagged: 72 (17.8%)\n",
      "  - High severity: 0\n",
      "  - Medium severity: 72\n",
      "  - Low severity: 0\n",
      "\n",
      "=== CREATING NEW NAVIGATION MAPS ===\n",
      "Using topology validation results for stop mapping\n",
      "Processing 917 unique data combinations\n",
      "Navigation maps created:\n",
      "  - 55 stops in combinations\n",
      "  - 1 routes\n",
      "  - 30 stop names mapped\n",
      "\n",
      "=== EXPORTING ALL DATA WITH GLOBAL MERGING ===\n",
      "Using global output folder: transit_analysis_global\n",
      "✅ master_indexer.json: Route cleanup + merge (2450 → 2854, +404 new)\n",
      "✅ log_pattern_violations.json: Route cleanup + merge (41 → 46, +5 new)\n",
      "✅ log_trip_types.json: Route cleanup + merge (2450 → 2854, +404 new)\n",
      "✅ log_regulatory_stops.json: Route cleanup + merge (542 → 627, +85 new)\n",
      "✅ global_stop_to_combinations.json: Global navigation accumulated (multi-route)\n",
      "✅ global_route_to_combinations.json: Global navigation accumulated (multi-route)\n",
      "✅ global_stop_name_to_stop_ids.json: Global navigation accumulated (multi-route)\n",
      "✅ global_route_short_name_to_info.json: Global navigation accumulated (multi-route)\n",
      "✅ Global summary updated: transit_analysis_global\\global_summary.json\n",
      "\n",
      "🌍 GLOBAL ANALYSIS STATUS:\n",
      "  - Routes analyzed: 11\n",
      "  - Stop names: 234\n",
      "  - Total combinations: 2854\n",
      "  - Issues detected: Yes\n",
      "  - Global files: transit_analysis_global\n",
      "  - Use master_indexer.json for detailed violation analysis\n",
      "processing raw data for route 215\n",
      "Removing 0 duplicates.\n",
      "\n",
      "=== STOP TOPOLOGY VALIDATION ===\n",
      "Built mapping for 18 stop names\n",
      "✅ Klarinettgatan: Valid mapping\n",
      "✅ Fagottgatan: Valid mapping\n",
      "✅ Fornvägen: Valid mapping\n",
      "✅ Ullstämmavägen: Valid mapping\n",
      "✅ Ullstämma by: Valid mapping\n",
      "✅ Morgongatan: Valid mapping\n",
      "✅ Trädgårdstorget: Valid mapping\n",
      "✅ Majelden: Valid mapping\n",
      "✅ Folkungavallen: Valid mapping\n",
      "✅ Tinnerbäcksbadet: Valid mapping\n",
      "✅ Myntgatan: Valid mapping\n",
      "✅ Grindgatan: Valid mapping\n",
      "✅ Kungsgatan: Valid mapping\n",
      "✅ Linköpings resecentrum: Valid mapping\n",
      "✅ Berga Söderleden: Valid mapping\n",
      "✅ Resedan: Valid mapping\n",
      "✅ Ridhusgatan: Valid mapping\n",
      "✅ Bergdalsgatan: Valid mapping\n",
      "Validation complete: 0 stop_id-direction violations detected (detailed logging deferred)\n",
      "\n",
      "=== UNIFIED TRIP CLASSIFICATION WITH GAP DETECTION ===\n",
      "Direction 1: canonical pattern 17 stops (from 4058 trips)\n",
      "Direction 0: canonical pattern 17 stops (from 4121 trips)\n",
      "Complete: 9456 trips, 2 stop-level violations affecting 20352 records\n",
      "Additional info: 20407 total non-full trip records\n",
      "\n",
      "=== FINALIZING TOPOLOGY VIOLATIONS LOG ===\n",
      "Finalized 0 detailed topology violations with trip_type granularity\n",
      "\n",
      "=== DETECTING REGULATORY STOPS ===\n",
      "Found 25 regulatory combinations\n",
      "Regulatory violations created: 0\n",
      "Regulatory analysis complete: 25 combinations, 0 violations\n",
      "\n",
      "=== CALCULATING TRAVEL TIMES AND DELAYS ===\n",
      "  Using stop-level pattern violations for travel time validation\n",
      "Calculation complete: 148722/159450 valid segments (93.3%)\n",
      "  Using stop-level pattern violations for precise validation\n",
      "\n",
      "=== CREATING MASTER INDEXER ===\n",
      "Processing 98 unique combinations\n",
      "Master indexer created: 98 combinations\n",
      "  - Flagged: 32 (32.7%)\n",
      "  - High severity: 0\n",
      "  - Medium severity: 32\n",
      "  - Low severity: 0\n",
      "\n",
      "=== CREATING NEW NAVIGATION MAPS ===\n",
      "Using topology validation results for stop mapping\n",
      "Processing 385 unique data combinations\n",
      "Navigation maps created:\n",
      "  - 33 stops in combinations\n",
      "  - 1 routes\n",
      "  - 18 stop names mapped\n",
      "\n",
      "=== EXPORTING ALL DATA WITH GLOBAL MERGING ===\n",
      "Using global output folder: transit_analysis_global\n",
      "✅ master_indexer.json: Route cleanup + merge (2854 → 2952, +98 new)\n",
      "✅ log_pattern_violations.json: Route cleanup + merge (46 → 48, +2 new)\n",
      "✅ log_trip_types.json: Route cleanup + merge (2854 → 2952, +98 new)\n",
      "✅ log_regulatory_stops.json: Route cleanup + merge (627 → 652, +25 new)\n",
      "✅ global_stop_to_combinations.json: Global navigation accumulated (multi-route)\n",
      "✅ global_route_to_combinations.json: Global navigation accumulated (multi-route)\n",
      "✅ global_stop_name_to_stop_ids.json: Global navigation accumulated (multi-route)\n",
      "✅ global_route_short_name_to_info.json: Global navigation accumulated (multi-route)\n",
      "✅ Global summary updated: transit_analysis_global\\global_summary.json\n",
      "\n",
      "🌍 GLOBAL ANALYSIS STATUS:\n",
      "  - Routes analyzed: 12\n",
      "  - Stop names: 252\n",
      "  - Total combinations: 2952\n",
      "  - Issues detected: Yes\n",
      "  - Global files: transit_analysis_global\n",
      "  - Use master_indexer.json for detailed violation analysis\n",
      "processing raw data for route 216\n",
      "Removing 0 duplicates.\n",
      "\n",
      "=== STOP TOPOLOGY VALIDATION ===\n",
      "Built mapping for 28 stop names\n",
      "✅ Ekdalsvägen: Valid mapping\n",
      "✅ Lövsbergsvägen: Valid mapping\n",
      "✅ Aspnäset: Valid mapping\n",
      "✅ Vårdsbergsvägen: Valid mapping\n",
      "✅ Hjulsbro skola: Valid mapping\n",
      "✅ Roshagsvägen: Valid mapping\n",
      "✅ Ljunghagsvägen: Valid mapping\n",
      "✅ Vårgård: Valid mapping\n",
      "✅ Hackefors södra: Valid mapping\n",
      "🚩 Hackefors: 1 violation(s) detected\n",
      "  - directional_contamination: 1 stop_ids × 2 directions\n",
      "✅ Landerydsvägen: Valid mapping\n",
      "✅ Saab Södra: Valid mapping\n",
      "✅ Saab Civila: Valid mapping\n",
      "✅ Stationsgatan: Valid mapping\n",
      "✅ Vetegatan: Valid mapping\n",
      "✅ Tannefors center: Valid mapping\n",
      "✅ Korngatan: Valid mapping\n",
      "✅ Humlegatan: Valid mapping\n",
      "✅ US Norra entrén: Valid mapping\n",
      "✅ Tinnerbäcksgränd: Valid mapping\n",
      "✅ Rotegatan: Valid mapping\n",
      "✅ US Södra entrén: Valid mapping\n",
      "✅ Pionjärgatan: Valid mapping\n",
      "✅ Berga centrum: Valid mapping\n",
      "✅ Tinnerbäcksbadet: Valid mapping\n",
      "✅ Linköpings resecentrum: Valid mapping\n",
      "✅ Kungsgatan: Valid mapping\n",
      "✅ Trädgårdstorget: Valid mapping\n",
      "🚩 Basic flags added: 5298 flagged, 0 critical records\n",
      "Validation complete: 2 stop_id-direction violations detected (detailed logging deferred)\n",
      "\n",
      "=== UNIFIED TRIP CLASSIFICATION WITH GAP DETECTION ===\n",
      "Direction 1: canonical pattern 28 stops (from 4100 trips)\n",
      "Direction 0: canonical pattern 28 stops (from 3539 trips)\n",
      "Complete: 9559 trips, 5 stop-level violations affecting 50654 records\n",
      "Additional info: 50906 total non-full trip records\n",
      "\n",
      "=== FINALIZING TOPOLOGY VIOLATIONS LOG ===\n",
      "Finalized 10 detailed topology violations with trip_type granularity\n",
      "\n",
      "=== DETECTING REGULATORY STOPS ===\n",
      "Found 63 regulatory combinations\n",
      "Regulatory violations created: 0\n",
      "Regulatory analysis complete: 63 combinations, 0 violations\n",
      "\n",
      "=== CALCULATING TRAVEL TIMES AND DELAYS ===\n",
      "  Using stop-level pattern violations for travel time validation\n",
      "Calculation complete: 253332/264798 valid segments (95.7%)\n",
      "  Using stop-level pattern violations for precise validation\n",
      "\n",
      "=== CREATING MASTER INDEXER ===\n",
      "Processing 358 unique combinations\n",
      "Master indexer created: 358 combinations\n",
      "  - Flagged: 77 (21.5%)\n",
      "  - High severity: 0\n",
      "  - Medium severity: 77\n",
      "  - Low severity: 0\n",
      "\n",
      "=== CREATING NEW NAVIGATION MAPS ===\n",
      "Using topology validation results for stop mapping\n",
      "Processing 959 unique data combinations\n",
      "Navigation maps created:\n",
      "  - 55 stops in combinations\n",
      "  - 1 routes\n",
      "  - 28 stop names mapped\n",
      "\n",
      "=== EXPORTING ALL DATA WITH GLOBAL MERGING ===\n",
      "Using global output folder: transit_analysis_global\n",
      "✅ master_indexer.json: Route cleanup + merge (2952 → 3310, +358 new)\n",
      "✅ log_topology_violations.json: Created new file (10 entries)\n",
      "✅ log_pattern_violations.json: Route cleanup + merge (48 → 53, +5 new)\n",
      "✅ log_trip_types.json: Route cleanup + merge (2952 → 3310, +358 new)\n",
      "✅ log_regulatory_stops.json: Route cleanup + merge (652 → 715, +63 new)\n",
      "✅ global_stop_to_combinations.json: Global navigation accumulated (multi-route)\n",
      "✅ global_route_to_combinations.json: Global navigation accumulated (multi-route)\n",
      "✅ global_stop_name_to_stop_ids.json: Global navigation accumulated (multi-route)\n",
      "✅ global_route_short_name_to_info.json: Global navigation accumulated (multi-route)\n",
      "✅ Global summary updated: transit_analysis_global\\global_summary.json\n",
      "\n",
      "🌍 GLOBAL ANALYSIS STATUS:\n",
      "  - Routes analyzed: 13\n",
      "  - Stop names: 280\n",
      "  - Total combinations: 3310\n",
      "  - Issues detected: Yes\n",
      "  - Global files: transit_analysis_global\n",
      "  - Use master_indexer.json for detailed violation analysis\n",
      "processing raw data for route 217\n",
      "Removing 0 duplicates.\n",
      "\n",
      "=== STOP TOPOLOGY VALIDATION ===\n",
      "Built mapping for 22 stop names\n",
      "✅ Aspnäset: Valid mapping\n",
      "✅ Ullstämma by: Valid mapping\n",
      "✅ Morgongatan: Valid mapping\n",
      "✅ Middagsgatan: Valid mapping\n",
      "✅ Strågatan: Valid mapping\n",
      "✅ Hässlegatan: Valid mapping\n",
      "✅ Ortgatan: Valid mapping\n",
      "✅ Poleraregatan: Valid mapping\n",
      "✅ Galoppgatan: Valid mapping\n",
      "✅ Yrkesvägen: Valid mapping\n",
      "✅ Lillgårdsgatan: Valid mapping\n",
      "✅ Berga centrum: Valid mapping\n",
      "✅ Rotegatan: Valid mapping\n",
      "✅ Pionjärgatan: Valid mapping\n",
      "✅ Kungsgatan: Valid mapping\n",
      "✅ Linköpings resecentrum: Valid mapping\n",
      "✅ Tinnerbäcksbadet: Valid mapping\n",
      "✅ Trädgårdstorget: Valid mapping\n",
      "✅ US Norra entrén: Valid mapping\n",
      "✅ Tinnerbäcksgränd: Valid mapping\n",
      "✅ US Södra entrén: Valid mapping\n",
      "✅ Berga Söderleden: Valid mapping\n",
      "Validation complete: 0 stop_id-direction violations detected (detailed logging deferred)\n",
      "\n",
      "=== UNIFIED TRIP CLASSIFICATION WITH GAP DETECTION ===\n",
      "Direction 1: canonical pattern 22 stops (from 3929 trips)\n",
      "Direction 0: canonical pattern 21 stops (from 4069 trips)\n",
      "Complete: 9251 trips, 3 stop-level violations affecting 25461 records\n",
      "Additional info: 25609 total non-full trip records\n",
      "\n",
      "=== FINALIZING TOPOLOGY VIOLATIONS LOG ===\n",
      "Finalized 0 detailed topology violations with trip_type granularity\n",
      "\n",
      "=== DETECTING REGULATORY STOPS ===\n",
      "Found 48 regulatory combinations\n",
      "Regulatory violations created: 0\n",
      "Regulatory analysis complete: 48 combinations, 0 violations\n",
      "\n",
      "=== CALCULATING TRAVEL TIMES AND DELAYS ===\n",
      "  Using stop-level pattern violations for travel time validation\n",
      "Calculation complete: 187002/197496 valid segments (94.7%)\n",
      "  Using stop-level pattern violations for precise validation\n",
      "\n",
      "=== CREATING MASTER INDEXER ===\n",
      "Processing 199 unique combinations\n",
      "Master indexer created: 199 combinations\n",
      "  - Flagged: 36 (18.1%)\n",
      "  - High severity: 0\n",
      "  - Medium severity: 36\n",
      "  - Low severity: 0\n",
      "\n",
      "=== CREATING NEW NAVIGATION MAPS ===\n",
      "Using topology validation results for stop mapping\n",
      "Processing 582 unique data combinations\n",
      "Navigation maps created:\n",
      "  - 41 stops in combinations\n",
      "  - 1 routes\n",
      "  - 22 stop names mapped\n",
      "\n",
      "=== EXPORTING ALL DATA WITH GLOBAL MERGING ===\n",
      "Using global output folder: transit_analysis_global\n",
      "✅ master_indexer.json: Route cleanup + merge (3310 → 3509, +199 new)\n",
      "✅ log_pattern_violations.json: Route cleanup + merge (53 → 56, +3 new)\n",
      "✅ log_trip_types.json: Route cleanup + merge (3310 → 3509, +199 new)\n",
      "✅ log_regulatory_stops.json: Route cleanup + merge (715 → 763, +48 new)\n",
      "✅ global_stop_to_combinations.json: Global navigation accumulated (multi-route)\n",
      "✅ global_route_to_combinations.json: Global navigation accumulated (multi-route)\n",
      "✅ global_stop_name_to_stop_ids.json: Global navigation accumulated (multi-route)\n",
      "✅ global_route_short_name_to_info.json: Global navigation accumulated (multi-route)\n",
      "✅ Global summary updated: transit_analysis_global\\global_summary.json\n",
      "\n",
      "🌍 GLOBAL ANALYSIS STATUS:\n",
      "  - Routes analyzed: 14\n",
      "  - Stop names: 302\n",
      "  - Total combinations: 3509\n",
      "  - Issues detected: Yes\n",
      "  - Global files: transit_analysis_global\n",
      "  - Use master_indexer.json for detailed violation analysis\n",
      "processing raw data for route 218\n",
      "Removing 0 duplicates.\n",
      "\n",
      "=== STOP TOPOLOGY VALIDATION ===\n",
      "Built mapping for 14 stop names\n",
      "✅ Gamla Linköping: Valid mapping\n",
      "✅ Vickergatan: Valid mapping\n",
      "✅ Linköpings resecentrum: Valid mapping\n",
      "✅ Kungsgatan: Valid mapping\n",
      "✅ Wernersgatan: Valid mapping\n",
      "✅ Westmansgatan 66: Valid mapping\n",
      "✅ Drabantgatan: Valid mapping\n",
      "✅ US Södra entrén: Valid mapping\n",
      "✅ US Norra entrén: Valid mapping\n",
      "✅ Eskadern: Valid mapping\n",
      "✅ Skogsfrid: Valid mapping\n",
      "✅ Fanjunkaregatan: Valid mapping\n",
      "✅ Tinnerbäcksbadet: Valid mapping\n",
      "✅ Trädgårdstorget: Valid mapping\n",
      "Validation complete: 0 stop_id-direction violations detected (detailed logging deferred)\n",
      "\n",
      "=== UNIFIED TRIP CLASSIFICATION WITH GAP DETECTION ===\n",
      "Direction 1: canonical pattern 14 stops (from 783 trips)\n",
      "Direction 0: canonical pattern 14 stops (from 800 trips)\n",
      "Complete: 9353 trips, 7 stop-level violations affecting 87904 records\n",
      "Additional info: 87940 total non-full trip records\n",
      "\n",
      "=== FINALIZING TOPOLOGY VIOLATIONS LOG ===\n",
      "Finalized 0 detailed topology violations with trip_type granularity\n",
      "\n",
      "=== DETECTING REGULATORY STOPS ===\n",
      "Found 31 regulatory combinations\n",
      "Regulatory violations created: 0\n",
      "Regulatory analysis complete: 31 combinations, 0 violations\n",
      "\n",
      "=== CALCULATING TRAVEL TIMES AND DELAYS ===\n",
      "  Using stop-level pattern violations for travel time validation\n",
      "Calculation complete: 92986/110102 valid segments (84.5%)\n",
      "  Using stop-level pattern violations for precise validation\n",
      "\n",
      "=== CREATING MASTER INDEXER ===\n",
      "Processing 124 unique combinations\n",
      "Master indexer created: 124 combinations\n",
      "  - Flagged: 72 (58.1%)\n",
      "  - High severity: 0\n",
      "  - Medium severity: 72\n",
      "  - Low severity: 0\n",
      "\n",
      "=== CREATING NEW NAVIGATION MAPS ===\n",
      "Using topology validation results for stop mapping\n",
      "Processing 434 unique data combinations\n",
      "Navigation maps created:\n",
      "  - 27 stops in combinations\n",
      "  - 1 routes\n",
      "  - 14 stop names mapped\n",
      "\n",
      "=== EXPORTING ALL DATA WITH GLOBAL MERGING ===\n",
      "Using global output folder: transit_analysis_global\n",
      "✅ master_indexer.json: Route cleanup + merge (3509 → 3633, +124 new)\n",
      "✅ log_pattern_violations.json: Route cleanup + merge (56 → 63, +7 new)\n",
      "✅ log_trip_types.json: Route cleanup + merge (3509 → 3633, +124 new)\n",
      "✅ log_regulatory_stops.json: Route cleanup + merge (763 → 794, +31 new)\n",
      "✅ global_stop_to_combinations.json: Global navigation accumulated (multi-route)\n",
      "✅ global_route_to_combinations.json: Global navigation accumulated (multi-route)\n",
      "✅ global_stop_name_to_stop_ids.json: Global navigation accumulated (multi-route)\n",
      "✅ global_route_short_name_to_info.json: Global navigation accumulated (multi-route)\n",
      "✅ Global summary updated: transit_analysis_global\\global_summary.json\n",
      "\n",
      "🌍 GLOBAL ANALYSIS STATUS:\n",
      "  - Routes analyzed: 15\n",
      "  - Stop names: 316\n",
      "  - Total combinations: 3633\n",
      "  - Issues detected: Yes\n",
      "  - Global files: transit_analysis_global\n",
      "  - Use master_indexer.json for detailed violation analysis\n",
      "processing raw data for route 220\n",
      "Removing 0 duplicates.\n",
      "\n",
      "=== STOP TOPOLOGY VALIDATION ===\n",
      "Built mapping for 6 stop names\n",
      "✅ Mjärdevi: Valid mapping\n",
      "✅ Forskningsbyn: Valid mapping\n",
      "✅ Mäster Mattias väg: Valid mapping\n",
      "✅ Universitetet: Valid mapping\n",
      "✅ Linköpings resecentrum: Valid mapping\n",
      "✅ Mjärdevi Center: Valid mapping\n",
      "Validation complete: 0 stop_id-direction violations detected (detailed logging deferred)\n",
      "\n",
      "=== UNIFIED TRIP CLASSIFICATION WITH GAP DETECTION ===\n",
      "Direction 1: canonical pattern 6 stops (from 705 trips)\n",
      "Direction 0: canonical pattern 6 stops (from 889 trips)\n",
      "Complete: 1595 trips, 0 stop-level violations affecting 0 records\n",
      "Additional info: 5 total non-full trip records\n",
      "\n",
      "=== FINALIZING TOPOLOGY VIOLATIONS LOG ===\n",
      "Finalized 0 detailed topology violations with trip_type granularity\n",
      "\n",
      "=== DETECTING REGULATORY STOPS ===\n",
      "Found 8 regulatory combinations\n",
      "Regulatory violations created: 0\n",
      "Regulatory analysis complete: 8 combinations, 0 violations\n",
      "\n",
      "=== CALCULATING TRAVEL TIMES AND DELAYS ===\n",
      "  Using stop-level pattern violations for travel time validation\n",
      "Calculation complete: 7974/9569 valid segments (83.3%)\n",
      "  Using stop-level pattern violations for precise validation\n",
      "\n",
      "=== CREATING MASTER INDEXER ===\n",
      "Processing 17 unique combinations\n",
      "Master indexer created: 17 combinations\n",
      "  - Flagged: 0 (0.0%)\n",
      "  - High severity: 0\n",
      "  - Medium severity: 0\n",
      "  - Low severity: 0\n",
      "\n",
      "=== CREATING NEW NAVIGATION MAPS ===\n",
      "Using topology validation results for stop mapping\n",
      "Processing 35 unique data combinations\n",
      "Navigation maps created:\n",
      "  - 11 stops in combinations\n",
      "  - 1 routes\n",
      "  - 6 stop names mapped\n",
      "\n",
      "=== EXPORTING ALL DATA WITH GLOBAL MERGING ===\n",
      "Using global output folder: transit_analysis_global\n",
      "✅ master_indexer.json: Route cleanup + merge (3633 → 3650, +17 new)\n",
      "✅ log_trip_types.json: Route cleanup + merge (3633 → 3650, +17 new)\n",
      "✅ log_regulatory_stops.json: Route cleanup + merge (794 → 802, +8 new)\n",
      "✅ global_stop_to_combinations.json: Global navigation accumulated (multi-route)\n",
      "✅ global_route_to_combinations.json: Global navigation accumulated (multi-route)\n",
      "✅ global_stop_name_to_stop_ids.json: Global navigation accumulated (multi-route)\n",
      "✅ global_route_short_name_to_info.json: Global navigation accumulated (multi-route)\n",
      "✅ Global summary updated: transit_analysis_global\\global_summary.json\n",
      "\n",
      "🌍 GLOBAL ANALYSIS STATUS:\n",
      "  - Routes analyzed: 16\n",
      "  - Stop names: 322\n",
      "  - Total combinations: 3650\n",
      "  - Issues detected: Yes\n",
      "  - Global files: transit_analysis_global\n",
      "  - Use master_indexer.json for detailed violation analysis\n",
      "processing raw data for route 221\n",
      "Removing 0 duplicates.\n",
      "\n",
      "=== STOP TOPOLOGY VALIDATION ===\n",
      "Built mapping for 7 stop names\n",
      "✅ Saab Civila: Valid mapping\n",
      "✅ Saab Södra: Valid mapping\n",
      "✅ Saab Norra porten: Valid mapping\n",
      "✅ Korngatan: Valid mapping\n",
      "✅ Humlegatan: Valid mapping\n",
      "✅ Linköpings resecentrum: Valid mapping\n",
      "✅ Landerydsvägen: Valid mapping\n",
      "Validation complete: 0 stop_id-direction violations detected (detailed logging deferred)\n",
      "\n",
      "=== UNIFIED TRIP CLASSIFICATION WITH GAP DETECTION ===\n",
      "Direction 1: canonical pattern 7 stops (from 703 trips)\n",
      "Direction 0: canonical pattern 7 stops (from 725 trips)\n",
      "Complete: 1431 trips, 0 stop-level violations affecting 0 records\n",
      "Additional info: 14 total non-full trip records\n",
      "\n",
      "=== FINALIZING TOPOLOGY VIOLATIONS LOG ===\n",
      "Finalized 0 detailed topology violations with trip_type granularity\n",
      "\n",
      "=== DETECTING REGULATORY STOPS ===\n",
      "Found 11 regulatory combinations\n",
      "Regulatory violations created: 0\n",
      "Regulatory analysis complete: 11 combinations, 0 violations\n",
      "\n",
      "=== CALCULATING TRAVEL TIMES AND DELAYS ===\n",
      "  Using stop-level pattern violations for travel time validation\n",
      "Calculation complete: 8579/10010 valid segments (85.7%)\n",
      "  Using stop-level pattern violations for precise validation\n",
      "\n",
      "=== CREATING MASTER INDEXER ===\n",
      "Processing 24 unique combinations\n",
      "Master indexer created: 24 combinations\n",
      "  - Flagged: 0 (0.0%)\n",
      "  - High severity: 0\n",
      "  - Medium severity: 0\n",
      "  - Low severity: 0\n",
      "\n",
      "=== CREATING NEW NAVIGATION MAPS ===\n",
      "Using topology validation results for stop mapping\n",
      "Processing 38 unique data combinations\n",
      "Navigation maps created:\n",
      "  - 13 stops in combinations\n",
      "  - 1 routes\n",
      "  - 7 stop names mapped\n",
      "\n",
      "=== EXPORTING ALL DATA WITH GLOBAL MERGING ===\n",
      "Using global output folder: transit_analysis_global\n",
      "✅ master_indexer.json: Route cleanup + merge (3650 → 3674, +24 new)\n",
      "✅ log_trip_types.json: Route cleanup + merge (3650 → 3674, +24 new)\n",
      "✅ log_regulatory_stops.json: Route cleanup + merge (802 → 813, +11 new)\n",
      "✅ global_stop_to_combinations.json: Global navigation accumulated (multi-route)\n",
      "✅ global_route_to_combinations.json: Global navigation accumulated (multi-route)\n",
      "✅ global_stop_name_to_stop_ids.json: Global navigation accumulated (multi-route)\n",
      "✅ global_route_short_name_to_info.json: Global navigation accumulated (multi-route)\n",
      "✅ Global summary updated: transit_analysis_global\\global_summary.json\n",
      "\n",
      "🌍 GLOBAL ANALYSIS STATUS:\n",
      "  - Routes analyzed: 17\n",
      "  - Stop names: 329\n",
      "  - Total combinations: 3674\n",
      "  - Issues detected: Yes\n",
      "  - Global files: transit_analysis_global\n",
      "  - Use master_indexer.json for detailed violation analysis\n",
      "processing raw data for route 222\n",
      "Removing 0 duplicates.\n",
      "\n",
      "=== STOP TOPOLOGY VALIDATION ===\n",
      "Built mapping for 28 stop names\n",
      "✅ Ekhaga: Valid mapping\n",
      "✅ Järdalavägen 50: Valid mapping\n",
      "✅ Ekholmens centrum: Valid mapping\n",
      "✅ Ektunavägen: Valid mapping\n",
      "✅ Kvinnebystigen: Valid mapping\n",
      "✅ Aspeliden: Valid mapping\n",
      "✅ Strandängsvägen: Valid mapping\n",
      "✅ Vårdsbergsvägen: Valid mapping\n",
      "✅ Ekdalsvägen: Valid mapping\n",
      "✅ Lövsbergsvägen: Valid mapping\n",
      "✅ Aspnäset: Valid mapping\n",
      "✅ Ullstämma by: Valid mapping\n",
      "✅ Morgongatan: Valid mapping\n",
      "✅ Grindgatan: Valid mapping\n",
      "✅ Myntgatan: Valid mapping\n",
      "✅ Resedan: Valid mapping\n",
      "✅ Ridhusgatan: Valid mapping\n",
      "✅ Berga centrum: Valid mapping\n",
      "✅ Pionjärgatan: Valid mapping\n",
      "✅ Nobeltorget: Valid mapping\n",
      "✅ Forskningsbyn: Valid mapping\n",
      "✅ Mjärdevi Center: Valid mapping\n",
      "✅ Rotegatan: Valid mapping\n",
      "✅ Mjärdevi: Valid mapping\n",
      "✅ Djurgården: Valid mapping\n",
      "✅ Garnisonen södra: Valid mapping\n",
      "✅ Vallastadens skola: Valid mapping\n",
      "✅ Berga Söderleden: Valid mapping\n",
      "Validation complete: 0 stop_id-direction violations detected (detailed logging deferred)\n",
      "\n",
      "=== UNIFIED TRIP CLASSIFICATION WITH GAP DETECTION ===\n",
      "Direction 1: canonical pattern 27 stops (from 391 trips)\n",
      "Direction 0: canonical pattern 28 stops (from 323 trips)\n",
      "Complete: 715 trips, 0 stop-level violations affecting 0 records\n",
      "Additional info: 9 total non-full trip records\n",
      "\n",
      "=== FINALIZING TOPOLOGY VIOLATIONS LOG ===\n",
      "Finalized 0 detailed topology violations with trip_type granularity\n",
      "\n",
      "=== DETECTING REGULATORY STOPS ===\n",
      "Found 13 regulatory combinations\n",
      "Regulatory violations created: 0\n",
      "Regulatory analysis complete: 13 combinations, 0 violations\n",
      "\n",
      "=== CALCULATING TRAVEL TIMES AND DELAYS ===\n",
      "  Using stop-level pattern violations for travel time validation\n",
      "Calculation complete: 18895/19610 valid segments (96.4%)\n",
      "  Using stop-level pattern violations for precise validation\n",
      "\n",
      "=== CREATING MASTER INDEXER ===\n",
      "Processing 64 unique combinations\n",
      "Master indexer created: 64 combinations\n",
      "  - Flagged: 0 (0.0%)\n",
      "  - High severity: 0\n",
      "  - Medium severity: 0\n",
      "  - Low severity: 0\n",
      "\n",
      "=== CREATING NEW NAVIGATION MAPS ===\n",
      "Using topology validation results for stop mapping\n",
      "Processing 92 unique data combinations\n",
      "Navigation maps created:\n",
      "  - 55 stops in combinations\n",
      "  - 1 routes\n",
      "  - 28 stop names mapped\n",
      "\n",
      "=== EXPORTING ALL DATA WITH GLOBAL MERGING ===\n",
      "Using global output folder: transit_analysis_global\n",
      "✅ master_indexer.json: Route cleanup + merge (3674 → 3738, +64 new)\n",
      "✅ log_trip_types.json: Route cleanup + merge (3674 → 3738, +64 new)\n",
      "✅ log_regulatory_stops.json: Route cleanup + merge (813 → 826, +13 new)\n",
      "✅ global_stop_to_combinations.json: Global navigation accumulated (multi-route)\n",
      "✅ global_route_to_combinations.json: Global navigation accumulated (multi-route)\n",
      "✅ global_stop_name_to_stop_ids.json: Global navigation accumulated (multi-route)\n",
      "✅ global_route_short_name_to_info.json: Global navigation accumulated (multi-route)\n",
      "✅ Global summary updated: transit_analysis_global\\global_summary.json\n",
      "\n",
      "🌍 GLOBAL ANALYSIS STATUS:\n",
      "  - Routes analyzed: 18\n",
      "  - Stop names: 357\n",
      "  - Total combinations: 3738\n",
      "  - Issues detected: Yes\n",
      "  - Global files: transit_analysis_global\n",
      "  - Use master_indexer.json for detailed violation analysis\n",
      "processing raw data for route 226\n",
      "Removing 0 duplicates.\n",
      "\n",
      "=== STOP TOPOLOGY VALIDATION ===\n",
      "Built mapping for 23 stop names\n",
      "✅ MAXI: Valid mapping\n",
      "✅ Norra Tornby: Valid mapping\n",
      "✅ IKEA: Valid mapping\n",
      "✅ Ullevi: Valid mapping\n",
      "✅ Skäggetorp centrum: Valid mapping\n",
      "✅ Mellangården: Valid mapping\n",
      "✅ Rusthållaregården: Valid mapping\n",
      "✅ Solhaga: Valid mapping\n",
      "✅ Ryd centrum: Valid mapping\n",
      "✅ Mårdtorpsgatan: Valid mapping\n",
      "🚩 Djursjukhuset: 2 violation(s) detected\n",
      "  - directional_contamination: 1 stop_ids × 2 directions\n",
      "  - directional_contamination: 1 stop_ids × 2 directions\n",
      "🚩 Tingsvägen: 2 violation(s) detected\n",
      "  - directional_contamination: 1 stop_ids × 2 directions\n",
      "  - directional_contamination: 1 stop_ids × 2 directions\n",
      "✅ Jägarvallsvägen: Valid mapping\n",
      "✅ Mjärdevi: Valid mapping\n",
      "✅ Mjärdevi Center: Valid mapping\n",
      "✅ Forskningsbyn: Valid mapping\n",
      "✅ Nobeltorget: Valid mapping\n",
      "✅ Boställsgatan: Valid mapping\n",
      "✅ Slestadsskolan: Valid mapping\n",
      "✅ Landbogatan: Valid mapping\n",
      "✅ Änggårdsskolan: Valid mapping\n",
      "✅ Isberget: Valid mapping\n",
      "✅ Rättaregatan: Valid mapping\n",
      "🚩 Basic flags added: 7788 flagged, 7788 critical records\n",
      "Validation complete: 8 stop_id-direction violations detected (detailed logging deferred)\n",
      "\n",
      "=== UNIFIED TRIP CLASSIFICATION WITH GAP DETECTION ===\n",
      "Direction 1: canonical pattern 25 stops (from 1028 trips)\n",
      "Direction 0: canonical pattern 25 stops (from 916 trips)\n",
      "Complete: 1948 trips, 0 stop-level violations affecting 0 records\n",
      "Additional info: 61 total non-full trip records\n",
      "\n",
      "=== FINALIZING TOPOLOGY VIOLATIONS LOG ===\n",
      "Finalized 16 detailed topology violations with trip_type granularity\n",
      "\n",
      "=== DETECTING REGULATORY STOPS ===\n",
      "Found 22 regulatory combinations\n",
      "Regulatory violations created: 0\n",
      "Regulatory analysis complete: 22 combinations, 0 violations\n",
      "\n",
      "=== CALCULATING TRAVEL TIMES AND DELAYS ===\n",
      "  Using stop-level pattern violations for travel time validation\n",
      "Calculation complete: 46713/48661 valid segments (96.0%)\n",
      "  Using stop-level pattern violations for precise validation\n",
      "\n",
      "=== CREATING MASTER INDEXER ===\n",
      "Processing 93 unique combinations\n",
      "Master indexer created: 93 combinations\n",
      "  - Flagged: 16 (17.2%)\n",
      "  - High severity: 16\n",
      "  - Medium severity: 0\n",
      "  - Low severity: 0\n",
      "\n",
      "=== CREATING NEW NAVIGATION MAPS ===\n",
      "Using topology validation results for stop mapping\n",
      "Processing 286 unique data combinations\n",
      "Navigation maps created:\n",
      "  - 43 stops in combinations\n",
      "  - 1 routes\n",
      "  - 23 stop names mapped\n",
      "\n",
      "=== EXPORTING ALL DATA WITH GLOBAL MERGING ===\n",
      "Using global output folder: transit_analysis_global\n",
      "✅ master_indexer.json: Route cleanup + merge (3738 → 3831, +93 new)\n",
      "✅ log_topology_violations.json: Route cleanup + merge (10 → 26, +16 new)\n",
      "✅ log_trip_types.json: Route cleanup + merge (3738 → 3831, +93 new)\n",
      "✅ log_regulatory_stops.json: Route cleanup + merge (826 → 848, +22 new)\n",
      "✅ global_stop_to_combinations.json: Global navigation accumulated (multi-route)\n",
      "✅ global_route_to_combinations.json: Global navigation accumulated (multi-route)\n",
      "✅ global_stop_name_to_stop_ids.json: Global navigation accumulated (multi-route)\n",
      "✅ global_route_short_name_to_info.json: Global navigation accumulated (multi-route)\n",
      "✅ Global summary updated: transit_analysis_global\\global_summary.json\n",
      "\n",
      "🌍 GLOBAL ANALYSIS STATUS:\n",
      "  - Routes analyzed: 19\n",
      "  - Stop names: 380\n",
      "  - Total combinations: 3831\n",
      "  - Issues detected: Yes\n",
      "  - Global files: transit_analysis_global\n",
      "  - Use master_indexer.json for detailed violation analysis\n"
     ]
    }
   ],
   "source": [
    "for i in num_list:\n",
    "    print(f'processing raw data for route {i}')\n",
    "    test_df = test_dataframes[i]\n",
    "    test_form = DataFormer(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created mockup DataFrame with 4614 records\n",
      "Unique stops: 7\n",
      "Unique stop_ids: 13\n",
      "Date range: 2024-03-15 00:00:00 to 2024-03-19 00:00:00\n",
      "Directions: [np.int64(0), np.int64(1)]\n",
      "\n",
      "🎯 EXPECTED VIOLATIONS:\n",
      "1. TOPOLOGY:\n",
      "   - University Campus: Directional contamination (same stop_id for both directions)\n",
      "   - City Hall: Missing shared stop (multiple directional stop_ids)\n",
      "2. PATTERN:\n",
      "   - partial_2 trips: Gap violations (missing intermediate stops)\n",
      "   - partial_3 trips: Swap + gap violations (out of order + missing stops)\n",
      "3. REGULATORY:\n",
      "   - Central Station & Medical Center: ~5% incomplete regulation\n",
      "\n",
      "📊 SAMPLE DATA:\n",
      "       trip_id  route_id route_short_name    route_long_name start_date  \\\n",
      "0  trip_000001       101             Blue  Blue Line Express 2024-03-15   \n",
      "1  trip_000001       101             Blue  Blue Line Express 2024-03-15   \n",
      "2  trip_000001       101             Blue  Blue Line Express 2024-03-15   \n",
      "3  trip_000001       101             Blue  Blue Line Express 2024-03-15   \n",
      "4  trip_000001       101             Blue  Blue Line Express 2024-03-15   \n",
      "5  trip_000001       101             Blue  Blue Line Express 2024-03-15   \n",
      "6  trip_000001       101             Blue  Blue Line Express 2024-03-15   \n",
      "7  trip_000002       101             Blue  Blue Line Express 2024-03-15   \n",
      "8  trip_000002       101             Blue  Blue Line Express 2024-03-15   \n",
      "9  trip_000002       101             Blue  Blue Line Express 2024-03-15   \n",
      "\n",
      "   direction_id  stop_id           stop_name  stop_sequence  \\\n",
      "0             0     1001     Central Station              1   \n",
      "1             0     1003  Financial District              2   \n",
      "2             0     2001   University Campus              3   \n",
      "3             0     1005   Business District              4   \n",
      "4             0     1007     Shopping Center              5   \n",
      "5             0     3001           City Hall              6   \n",
      "6             0     1009      Medical Center              7   \n",
      "7             0     1001     Central Station              1   \n",
      "8             0     1003  Financial District              2   \n",
      "9             0     2001   University Campus              3   \n",
      "\n",
      "  scheduled_departure_time observed_departure_time  departure_delay      city  \n",
      "0      2024-03-15 07:01:00     2024-03-15 07:01:39               39  TestCity  \n",
      "1      2024-03-15 07:04:00     2024-03-15 07:05:19               79  TestCity  \n",
      "2      2024-03-15 07:07:00     2024-03-15 07:07:56               56  TestCity  \n",
      "3      2024-03-15 07:09:00     2024-03-15 07:10:07               67  TestCity  \n",
      "4      2024-03-15 07:13:00     2024-03-15 07:15:22              142  TestCity  \n",
      "5      2024-03-15 07:16:00     2024-03-15 07:17:14               74  TestCity  \n",
      "6      2024-03-15 07:19:00     2024-03-15 07:20:30               90  TestCity  \n",
      "7      2024-03-15 07:01:15     2024-03-15 07:01:59               44  TestCity  \n",
      "8      2024-03-15 07:05:00     2024-03-15 07:06:37               97  TestCity  \n",
      "9      2024-03-15 07:07:00     2024-03-15 07:07:00                0  TestCity  \n",
      "\n",
      "🔍 TOPOLOGY CHECK - University Campus:\n",
      "Stop_id 2001 usage by direction:\n",
      "  Direction 0: 391 records\n",
      "  Direction 1: 399 records\n",
      "\n",
      "🔍 PATTERN CHECK - Trip patterns by type:\n",
      "\n",
      "Direction 0 patterns:\n",
      "  Pattern 1: [1001, 1003, 2001, 1005, 1007, 3001, 1009] (200 trips)\n",
      "  Pattern 2: [1001, 1003, 2001, 1005, 1007] (115 trips)\n",
      "  Pattern 3: [1001, 2001, 1007, 1009] (58 trips)\n",
      "  Pattern 4: [1001, 1007, 2001, 1009] (18 trips)\n",
      "\n",
      "Direction 1 patterns:\n",
      "  Pattern 1: [1002, 1004, 2001, 1006, 1008, 3002, 1010] (208 trips)\n",
      "  Pattern 2: [1002, 1004, 2001, 1006, 1008] (115 trips)\n",
      "  Pattern 3: [1002, 2001, 1008, 1010] (58 trips)\n",
      "  Pattern 4: [1002, 1008, 2001, 1010] (18 trips)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def create_mockup_dataframe():\n",
    "    \"\"\"Create a realistic DataFrame that demonstrates all violation types\"\"\"\n",
    "    \n",
    "    # Base data structure\n",
    "    data = []\n",
    "    \n",
    "    # Route info\n",
    "    route_id = 101\n",
    "    route_short_name = \"Blue\"\n",
    "    route_long_name = \"Blue Line Express\"\n",
    "    base_date = datetime(2024, 3, 15)  # Friday\n",
    "    \n",
    "    # Stop definitions with different topology scenarios\n",
    "    stops_config = [\n",
    "        # Normal bidirectional stops (separate stop_ids per direction)\n",
    "        {\"name\": \"Central Station\", \"stop_ids\": {0: 1001, 1: 1002}, \"sequence\": {0: 1, 1: 1}},\n",
    "        {\"name\": \"Financial District\", \"stop_ids\": {0: 1003, 1: 1004}, \"sequence\": {0: 2, 1: 2}},\n",
    "        \n",
    "        # TOPOLOGY VIOLATION 1: Directional contamination\n",
    "        {\"name\": \"University Campus\", \"stop_ids\": {0: 2001, 1: 2001}, \"sequence\": {0: 3, 1: 3}, \n",
    "         \"contamination\": True},  # Same stop_id for both directions\n",
    "        \n",
    "        {\"name\": \"Business District\", \"stop_ids\": {0: 1005, 1: 1006}, \"sequence\": {0: 4, 1: 4}},\n",
    "        {\"name\": \"Shopping Center\", \"stop_ids\": {0: 1007, 1: 1008}, \"sequence\": {0: 5, 1: 5}},\n",
    "        \n",
    "        # TOPOLOGY VIOLATION 2: Missing shared stop (3 stop_ids, all directional)\n",
    "        {\"name\": \"City Hall\", \"stop_ids\": {0: 3001, 1: [3002, 3003]}, \"sequence\": {0: 6, 1: 6}},\n",
    "        \n",
    "        {\"name\": \"Medical Center\", \"stop_ids\": {0: 1009, 1: 1010}, \"sequence\": {0: 7, 1: 7}},\n",
    "    ]\n",
    "    \n",
    "    # Trip types and their patterns\n",
    "    trip_patterns = {\n",
    "        0: {  # Direction 0\n",
    "            \"full\": [1001, 1003, 2001, 1005, 1007, 3001, 1009],\n",
    "            \"partial_1\": [1001, 1003, 2001, 1005, 1007],  # Ends early\n",
    "            \"partial_2\": [1001, 2001, 1007, 1009],        # PATTERN VIOLATION: Has gaps\n",
    "            \"partial_3\": [1001, 1007, 2001, 1009]         # PATTERN VIOLATION: Has swaps + gaps\n",
    "        },\n",
    "        1: {  # Direction 1  \n",
    "            \"full\": [1002, 1004, 2001, 1006, 1008, 3002, 1010],\n",
    "            \"partial_1\": [1002, 1004, 2001, 1006, 1008],\n",
    "            \"partial_2\": [1002, 2001, 1008, 1010],        # Has gaps\n",
    "            \"partial_3\": [1002, 1008, 2001, 1010]         # Has swaps + gaps\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Generate trips over multiple days\n",
    "    trip_id_counter = 1\n",
    "    \n",
    "    for day_offset in range(5):  # 5 days of data\n",
    "        current_date = base_date + timedelta(days=day_offset)\n",
    "        \n",
    "        # Determine day type\n",
    "        weekday = current_date.weekday()\n",
    "        if weekday <= 4:\n",
    "            day_type = 'weekday' \n",
    "            trip_counts = {\"full\": 50, \"partial_1\": 30, \"partial_2\": 15, \"partial_3\": 5}\n",
    "        elif weekday == 5:\n",
    "            day_type = 'saturday'\n",
    "            trip_counts = {\"full\": 30, \"partial_1\": 15, \"partial_2\": 8, \"partial_3\": 2}\n",
    "        else:\n",
    "            day_type = 'sunday' \n",
    "            trip_counts = {\"full\": 20, \"partial_1\": 10, \"partial_2\": 5, \"partial_3\": 1}\n",
    "        \n",
    "        # Generate trips for each direction and pattern\n",
    "        for direction in [0, 1]:\n",
    "            for trip_type, pattern in trip_patterns[direction].items():\n",
    "                trip_count = trip_counts[trip_type]\n",
    "                \n",
    "                # Add contamination bias for University Campus\n",
    "                if direction == 1 and \"University Campus\" in [s[\"name\"] for s in stops_config]:\n",
    "                    if trip_type == \"full\":\n",
    "                        # Add some trips that should be direction 0 but recorded as direction 1\n",
    "                        contamination_trips = max(1, trip_count // 20)  # ~5% contamination\n",
    "                        trip_count += contamination_trips\n",
    "                \n",
    "                for trip_num in range(trip_count):\n",
    "                    trip_id = f\"trip_{trip_id_counter:06d}\"\n",
    "                    trip_id_counter += 1\n",
    "                    \n",
    "                    # Determine time category based on trip number\n",
    "                    if trip_num < trip_count * 0.2:\n",
    "                        time_cat = 'am_rush'\n",
    "                        base_hour = 7\n",
    "                    elif trip_num < trip_count * 0.4:\n",
    "                        time_cat = 'day'\n",
    "                        base_hour = 11\n",
    "                    elif trip_num < trip_count * 0.6:\n",
    "                        time_cat = 'pm_rush' \n",
    "                        base_hour = 17\n",
    "                    elif trip_num < trip_count * 0.8:\n",
    "                        time_cat = 'night'\n",
    "                        base_hour = 21\n",
    "                    else:\n",
    "                        time_cat = 'weekend'\n",
    "                        base_hour = 14\n",
    "                    \n",
    "                    # Override for weekends\n",
    "                    if day_type in ['saturday', 'sunday']:\n",
    "                        time_cat = 'weekend'\n",
    "                        base_hour = 10 + (trip_num % 8)\n",
    "                    \n",
    "                    # Generate stop records for this trip\n",
    "                    base_time = current_date.replace(hour=base_hour, minute=trip_num % 60, second=0)\n",
    "                    \n",
    "                    for seq, stop_id in enumerate(pattern):\n",
    "                        # Find stop info\n",
    "                        stop_info = None\n",
    "                        for stop in stops_config:\n",
    "                            if isinstance(stop[\"stop_ids\"][direction], list):\n",
    "                                if stop_id in stop[\"stop_ids\"][direction]:\n",
    "                                    stop_info = stop\n",
    "                                    break\n",
    "                            else:\n",
    "                                if stop_id == stop[\"stop_ids\"][direction] or stop_id == stop[\"stop_ids\"].get(1-direction):\n",
    "                                    stop_info = stop\n",
    "                                    break\n",
    "                        \n",
    "                        if stop_info is None:\n",
    "                            continue\n",
    "                            \n",
    "                        stop_name = stop_info[\"name\"]\n",
    "                        stop_sequence = seq + 1\n",
    "                        \n",
    "                        # Calculate times\n",
    "                        minutes_offset = seq * 3 + np.random.randint(0, 2)  # 3 min between stops + variation\n",
    "                        scheduled_time = base_time + timedelta(minutes=minutes_offset)\n",
    "                        \n",
    "                        # Add delay variation\n",
    "                        base_delay = np.random.normal(30, 60)  # 30s mean, 60s std\n",
    "                        if time_cat == 'am_rush':\n",
    "                            base_delay += np.random.normal(45, 30)  # Rush hour delays\n",
    "                        elif time_cat == 'pm_rush':\n",
    "                            base_delay += np.random.normal(60, 45)\n",
    "                        \n",
    "                        departure_delay = max(0, int(base_delay))\n",
    "                        \n",
    "                        # REGULATORY VIOLATION: Some stops should be regulated\n",
    "                        if stop_name in [\"Central Station\", \"Medical Center\"] and trip_type == \"full\":\n",
    "                            # 95% regulated (should be 100%)\n",
    "                            if np.random.random() < 0.95:\n",
    "                                # Force to :00 seconds for regulation\n",
    "                                scheduled_time = scheduled_time.replace(second=0)\n",
    "                            else:\n",
    "                                # VIOLATION: Not regulated \n",
    "                                scheduled_time = scheduled_time.replace(second=np.random.choice([15, 30, 45]))\n",
    "                        \n",
    "                        observed_time = scheduled_time + timedelta(seconds=departure_delay)\n",
    "                        \n",
    "                        # Create record\n",
    "                        record = {\n",
    "                            'trip_id': trip_id,\n",
    "                            'route_id': route_id,\n",
    "                            'route_short_name': route_short_name,\n",
    "                            'route_long_name': route_long_name,\n",
    "                            'start_date': current_date.strftime('%Y%m%d'),\n",
    "                            'direction_id': direction,\n",
    "                            'stop_id': stop_id,\n",
    "                            'stop_name': stop_name,\n",
    "                            'stop_sequence': stop_sequence,\n",
    "                            'scheduled_departure_time': scheduled_time,\n",
    "                            'observed_departure_time': observed_time,\n",
    "                            'departure_delay': departure_delay,\n",
    "                            'city': 'TestCity'\n",
    "                        }\n",
    "                        \n",
    "                        data.append(record)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Ensure proper data types\n",
    "    df['start_date'] = pd.to_datetime(df['start_date'], format='%Y%m%d')\n",
    "    df['scheduled_departure_time'] = pd.to_datetime(df['scheduled_departure_time'])\n",
    "    df['observed_departure_time'] = pd.to_datetime(df['observed_departure_time'])\n",
    "    df['route_id'] = df['route_id'].astype(int)\n",
    "    df['direction_id'] = df['direction_id'].astype(int)\n",
    "    df['stop_id'] = df['stop_id'].astype(int)\n",
    "    df['stop_sequence'] = df['stop_sequence'].astype(int)\n",
    "    df['departure_delay'] = df['departure_delay'].astype(int)\n",
    "    \n",
    "    print(f\"Created mockup DataFrame with {len(df)} records\")\n",
    "    print(f\"Unique stops: {df['stop_name'].nunique()}\")\n",
    "    print(f\"Unique stop_ids: {df['stop_id'].nunique()}\")\n",
    "    print(f\"Date range: {df['start_date'].min()} to {df['start_date'].max()}\")\n",
    "    print(f\"Directions: {sorted(df['direction_id'].unique())}\")\n",
    "    \n",
    "    # Show expected violations\n",
    "    print(\"\\n🎯 EXPECTED VIOLATIONS:\")\n",
    "    print(\"1. TOPOLOGY:\")\n",
    "    print(\"   - University Campus: Directional contamination (same stop_id for both directions)\")\n",
    "    print(\"   - City Hall: Missing shared stop (multiple directional stop_ids)\")\n",
    "    print(\"2. PATTERN:\")\n",
    "    print(\"   - partial_2 trips: Gap violations (missing intermediate stops)\")\n",
    "    print(\"   - partial_3 trips: Swap + gap violations (out of order + missing stops)\")\n",
    "    print(\"3. REGULATORY:\")\n",
    "    print(\"   - Central Station & Medical Center: ~5% incomplete regulation\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate the test data\n",
    "mockup_df = create_mockup_dataframe()\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\n📊 SAMPLE DATA:\")\n",
    "print(mockup_df.head(10))\n",
    "\n",
    "print(\"\\n🔍 TOPOLOGY CHECK - University Campus:\")\n",
    "campus_data = mockup_df[mockup_df['stop_name'] == 'University Campus']\n",
    "print(f\"Stop_id 2001 usage by direction:\")\n",
    "for direction in sorted(campus_data['direction_id'].unique()):\n",
    "    count = len(campus_data[campus_data['direction_id'] == direction])\n",
    "    print(f\"  Direction {direction}: {count} records\")\n",
    "\n",
    "print(\"\\n🔍 PATTERN CHECK - Trip patterns by type:\")\n",
    "sample_trips = mockup_df.groupby(['direction_id', 'trip_id']).agg({\n",
    "    'stop_id': lambda x: list(x),\n",
    "    'stop_name': 'count'\n",
    "}).rename(columns={'stop_name': 'stop_count'}).reset_index()\n",
    "\n",
    "for direction in [0, 1]:\n",
    "    dir_trips = sample_trips[sample_trips['direction_id'] == direction]\n",
    "    patterns = dir_trips['stop_id'].apply(tuple).value_counts()\n",
    "    print(f\"\\nDirection {direction} patterns:\")\n",
    "    for i, (pattern, count) in enumerate(patterns.head(4).items()):\n",
    "        print(f\"  Pattern {i+1}: {list(pattern)} ({count} trips)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created mockup DataFrame with 4614 records\n",
      "Unique stops: 7\n",
      "Unique stop_ids: 13\n",
      "Date range: 2024-03-15 00:00:00 to 2024-03-19 00:00:00\n",
      "Directions: [np.int64(0), np.int64(1)]\n",
      "\n",
      "🎯 EXPECTED VIOLATIONS:\n",
      "1. TOPOLOGY:\n",
      "   - University Campus: Directional contamination (same stop_id for both directions)\n",
      "   - City Hall: Missing shared stop (multiple directional stop_ids)\n",
      "2. PATTERN:\n",
      "   - partial_2 trips: Gap violations (missing intermediate stops)\n",
      "   - partial_3 trips: Swap + gap violations (out of order + missing stops)\n",
      "3. REGULATORY:\n",
      "   - Central Station & Medical Center: ~5% incomplete regulation\n",
      "\n",
      "==================================================\n",
      "🧪 TESTING DATAFORMER WITH MOCKUP DATA\n",
      "==================================================\n",
      "Removing 0 duplicates.\n",
      "\n",
      "=== STOP TOPOLOGY VALIDATION ===\n",
      "Built mapping for 7 stop names\n",
      "✅ Central Station: Valid mapping\n",
      "✅ Financial District: Valid mapping\n",
      "✅ University Campus: Valid mapping\n",
      "✅ Business District: Valid mapping\n",
      "✅ Shopping Center: Valid mapping\n",
      "✅ City Hall: Valid mapping\n",
      "✅ Medical Center: Valid mapping\n",
      "Validation complete: 0 stop_id-direction violations detected (detailed logging deferred)\n",
      "\n",
      "=== UNIFIED TRIP CLASSIFICATION WITH GAP DETECTION ===\n",
      "Direction 0: canonical pattern 7 stops (from 200 trips)\n",
      "Direction 1: canonical pattern 7 stops (from 208 trips)\n",
      "Complete: 790 trips, 6 stop-level violations affecting 608 records\n",
      "Additional info: 1758 total non-full trip records\n",
      "\n",
      "=== FINALIZING TOPOLOGY VIOLATIONS LOG ===\n",
      "Finalized 0 detailed topology violations with trip_type granularity\n",
      "\n",
      "=== DETECTING REGULATORY STOPS ===\n",
      "Found 30 regulatory combinations\n",
      "  - 2 with anomalies\n",
      "Regulatory violations created: 2\n",
      "Regulatory analysis complete: 30 combinations, 2 violations\n",
      "\n",
      "=== CALCULATING TRAVEL TIMES AND DELAYS ===\n",
      "  Using stop-level pattern violations for travel time validation\n",
      "Calculation complete: 3368/4614 valid segments (73.0%)\n",
      "  Using stop-level pattern violations for precise validation\n",
      "\n",
      "=== CREATING MASTER INDEXER ===\n",
      "Processing 32 unique combinations\n",
      "Master indexer created: 32 combinations\n",
      "  - Flagged: 10 (31.2%)\n",
      "  - High severity: 8\n",
      "  - Medium severity: 0\n",
      "  - Low severity: 2\n",
      "\n",
      "=== CREATING NEW NAVIGATION MAPS ===\n",
      "Using topology validation results for stop mapping\n",
      "Processing 128 unique data combinations\n",
      "Navigation maps created:\n",
      "  - 13 stops in combinations\n",
      "  - 1 routes\n",
      "  - 7 stop names mapped\n",
      "\n",
      "=== EXPORTING ALL DATA WITH GLOBAL MERGING ===\n",
      "Using global output folder: transit_analysis_global\n",
      "✅ master_indexer.json: Created new file (32 entries)\n",
      "✅ log_pattern_violations.json: Created new file (6 entries)\n",
      "✅ log_regulatory_violations.json: Created new file (2 entries)\n",
      "✅ log_trip_types.json: Created new file (32 entries)\n",
      "✅ log_regulatory_stops.json: Created new file (30 entries)\n",
      "✅ global_stop_to_combinations.json: Created global navigation file\n",
      "✅ global_route_to_combinations.json: Created global navigation file\n",
      "✅ global_stop_name_to_stop_ids.json: Created global navigation file\n",
      "✅ global_route_short_name_to_info.json: Created global navigation file\n",
      "✅ Global summary updated: transit_analysis_global\\global_summary.json\n",
      "\n",
      "🌍 GLOBAL ANALYSIS STATUS:\n",
      "  - Routes analyzed: 1\n",
      "  - Stop names: 7\n",
      "  - Total combinations: 32\n",
      "  - Issues detected: Yes\n",
      "  - Global files: transit_analysis_global\n",
      "  - Use master_indexer.json for detailed violation analysis\n",
      "\n",
      "==================================================\n",
      "📊 ANALYSIS RESULTS\n",
      "==================================================\n",
      "Master indexer entries: 32\n",
      "Topology violations: 0\n",
      "Pattern violations: 6\n",
      "Regulatory violations: 2\n",
      "Flagged combinations: 10/32\n",
      "Severity breakdown:\n",
      "  high: 8\n",
      "  low: 2\n",
      "  none: 22\n",
      "\n",
      "🚩 PATTERN VIOLATIONS DETECTED:\n",
      "  University Campus (partial_1): gap_before_stop\n",
      "  Shopping Center (partial_1): gap_before_stop\n",
      "  Medical Center (partial_1): gap_before_stop\n",
      "\n",
      "🚩 REGULATORY VIOLATIONS DETECTED:\n",
      "  Central Station (full): incomplete_regulation\n",
      "  Medical Center (full): incomplete_regulation\n",
      "\n",
      "✅ Test completed! Check the global output files for detailed results.\n"
     ]
    }
   ],
   "source": [
    "# Run this to test your DataFormer class with the mockup data:\n",
    "\n",
    "# 1. Generate the mockup data\n",
    "mockup_df = create_mockup_dataframe()\n",
    "\n",
    "# 2. Test with your DataFormer class\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"🧪 TESTING DATAFORMER WITH MOCKUP DATA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "test_former = DataFormer(mockup_df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"📊 ANALYSIS RESULTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check what violations were detected\n",
    "print(f\"Master indexer entries: {len(test_former._master_indexer)}\")\n",
    "print(f\"Topology violations: {len(test_former._topology_violations_log)}\")\n",
    "print(f\"Pattern violations: {len(test_former._pattern_violations_log)}\")\n",
    "print(f\"Regulatory violations: {len(test_former._regulatory_violations_log)}\")\n",
    "\n",
    "# Show severity breakdown\n",
    "flagged = sum(1 for entry in test_former._master_indexer.values() \n",
    "              if entry['violation_flags']['has_any_violation'])\n",
    "print(f\"Flagged combinations: {flagged}/{len(test_former._master_indexer)}\")\n",
    "\n",
    "severity_counts = {'high': 0, 'medium': 0, 'low': 0, 'none': 0}\n",
    "for entry in test_former._master_indexer.values():\n",
    "    severity = entry.get('overall_severity', 'none')\n",
    "    severity_counts[severity] += 1\n",
    "\n",
    "print(\"Severity breakdown:\")\n",
    "for severity, count in severity_counts.items():\n",
    "    if count > 0:\n",
    "        print(f\"  {severity}: {count}\")\n",
    "\n",
    "# Show sample violations\n",
    "if test_former._topology_violations_log:\n",
    "    print(\"\\n🚩 TOPOLOGY VIOLATIONS DETECTED:\")\n",
    "    for key, violation in list(test_former._topology_violations_log.items())[:3]:\n",
    "        print(f\"  {violation['stop_name']}: {violation['violation_type']} ({violation['severity']})\")\n",
    "\n",
    "if test_former._pattern_violations_log:\n",
    "    print(\"\\n🚩 PATTERN VIOLATIONS DETECTED:\")\n",
    "    for key, violation in list(test_former._pattern_violations_log.items())[:3]:\n",
    "        print(f\"  {violation['stop_name']} ({violation['trip_type']}): {violation['violation_type']}\")\n",
    "\n",
    "if test_former._regulatory_violations_log:\n",
    "    print(\"\\n🚩 REGULATORY VIOLATIONS DETECTED:\")\n",
    "    for key, violation in list(test_former._regulatory_violations_log.items())[:3]:\n",
    "        print(f\"  {violation['stop_name']} ({violation['trip_type']}): {violation['violation_type']}\")\n",
    "\n",
    "print(\"\\n✅ Test completed! Check the global output files for detailed results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TOPOLOGY CHECK ===\n",
      "University Campus stop_id usage:\n",
      "direction_id  stop_id\n",
      "0             2001       391\n",
      "1             2001       399\n",
      "dtype: int64\n",
      "\n",
      "=== PATTERN CHECK ===\n",
      "Sample trip patterns:\n",
      "Direction 0, Trip trip_000001: [1001, 1003, 2001, 1005, 1007, 3001, 1009]\n",
      "Direction 0, Trip trip_000002: [1001, 1003, 2001, 1005, 1007, 3001, 1009]\n",
      "Direction 0, Trip trip_000003: [1001, 1003, 2001, 1005, 1007, 3001, 1009]\n",
      "Direction 0, Trip trip_000004: [1001, 1003, 2001, 1005, 1007, 3001, 1009]\n",
      "Direction 0, Trip trip_000005: [1001, 1003, 2001, 1005, 1007, 3001, 1009]\n",
      "Direction 0, Trip trip_000006: [1001, 1003, 2001, 1005, 1007, 3001, 1009]\n",
      "Direction 0, Trip trip_000007: [1001, 1003, 2001, 1005, 1007, 3001, 1009]\n",
      "Direction 0, Trip trip_000008: [1001, 1003, 2001, 1005, 1007, 3001, 1009]\n",
      "Direction 0, Trip trip_000009: [1001, 1003, 2001, 1005, 1007, 3001, 1009]\n",
      "Direction 0, Trip trip_000010: [1001, 1003, 2001, 1005, 1007, 3001, 1009]\n"
     ]
    }
   ],
   "source": [
    "# Check the actual data structure\n",
    "print(\"=== TOPOLOGY CHECK ===\")\n",
    "campus_data = mockup_df[mockup_df['stop_name'] == 'University Campus']\n",
    "print(\"University Campus stop_id usage:\")\n",
    "print(campus_data.groupby(['direction_id', 'stop_id']).size())\n",
    "\n",
    "print(\"\\n=== PATTERN CHECK ===\") \n",
    "sample_trips = mockup_df.groupby(['direction_id', 'trip_id']).agg({\n",
    "    'stop_id': lambda x: list(x)\n",
    "}).head(10)\n",
    "print(\"Sample trip patterns:\")\n",
    "for idx, row in sample_trips.iterrows():\n",
    "    print(f\"Direction {idx[0]}, Trip {idx[1]}: {row['stop_id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STOP TOPOLOGY MAPPING CHECK ===\n",
      "Stop name to stop_ids mapping:\n",
      "{'Central Station': ['1001', '1002'], 'Financial District': ['1003', '1004'], 'University Campus': ['2001'], 'Business District': ['1005', '1006'], 'Shopping Center': ['1007', '1008'], 'City Hall': ['3001', '3002'], 'Medical Center': ['1009', '1010']}\n",
      "\n",
      "=== UNIVERSITY CAMPUS SPECIFIC CHECK ===\n",
      "University Campus mapped to stop_ids: ['2001']\n",
      "Number of stop_ids: 1\n",
      "✅ CORRECT: Identified as shared stop (1 stop_id for both directions)\n"
     ]
    }
   ],
   "source": [
    "print(\"=== STOP TOPOLOGY MAPPING CHECK ===\")\n",
    "print(\"Stop name to stop_ids mapping:\")\n",
    "print(test_former._stop_name_to_stop_ids)\n",
    "\n",
    "print(\"\\n=== UNIVERSITY CAMPUS SPECIFIC CHECK ===\")\n",
    "if 'University Campus' in test_former._stop_name_to_stop_ids:\n",
    "    campus_stop_ids = test_former._stop_name_to_stop_ids['University Campus']\n",
    "    print(f\"University Campus mapped to stop_ids: {campus_stop_ids}\")\n",
    "    print(f\"Number of stop_ids: {len(campus_stop_ids)}\")\n",
    "    \n",
    "    if len(campus_stop_ids) == 1:\n",
    "        print(\"✅ CORRECT: Identified as shared stop (1 stop_id for both directions)\")\n",
    "    else:\n",
    "        print(\"❌ INCORRECT: Should be 1 stop_id, but found multiple\")\n",
    "else:\n",
    "    print(\"❌ University Campus not found in mapping!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              trip_id          route_id start_date  direction_id  \\\n",
      "0   55700000059723173  9011005020100000 2021-09-01             1   \n",
      "1   55700000059723173  9011005020100000 2021-09-01             1   \n",
      "2   55700000059723173  9011005020100000 2021-09-01             1   \n",
      "3   55700000059723173  9011005020100000 2021-09-01             1   \n",
      "4   55700000059723173  9011005020100000 2021-09-01             1   \n",
      "..                ...               ...        ...           ...   \n",
      "95  55700000059723173  9011005020100000 2021-09-15             1   \n",
      "96  55700000059723173  9011005020100000 2021-09-15             1   \n",
      "97  55700000059723173  9011005020100000 2021-09-15             1   \n",
      "98  55700000059723173  9011005020100000 2021-09-15             1   \n",
      "99  55700000059723173  9011005020100000 2021-09-16             1   \n",
      "\n",
      "             stop_id               stop_name  stop_sequence  \\\n",
      "0   9022005000048001      Skäggetorp centrum            1.0   \n",
      "1   9022005001411001            Mellangården            2.0   \n",
      "2   9022005001236001       Rusthållaregården            3.0   \n",
      "3   9022005001235001               Blomgatan            4.0   \n",
      "4   9022005001234001                 Åbylund            5.0   \n",
      "..               ...                     ...            ...   \n",
      "95  9022005001219001                  Abisko            6.0   \n",
      "96  9022005000417001      Konsert & Kongress            7.0   \n",
      "97  9022005001223001            Klostergatan            8.0   \n",
      "98  9022005000050002  Linköpings resecentrum            9.0   \n",
      "99  9022005000048001      Skäggetorp centrum            1.0   \n",
      "\n",
      "   scheduled_departure_time observed_departure_time  departure_delay  \\\n",
      "0       2021-09-01 07:33:00     2021-09-01 07:34:25             85.0   \n",
      "1       2021-09-01 07:35:49     2021-09-01 07:36:16             27.0   \n",
      "2       2021-09-01 07:36:54     2021-09-01 07:37:24             30.0   \n",
      "3       2021-09-01 07:39:25     2021-09-01 07:39:01            -24.0   \n",
      "4       2021-09-01 07:41:01     2021-09-01 07:39:59            -62.0   \n",
      "..                      ...                     ...              ...   \n",
      "95      2021-09-15 07:43:04     2021-09-15 07:41:31            -93.0   \n",
      "96      2021-09-15 07:45:04     2021-09-15 07:43:22           -102.0   \n",
      "97      2021-09-15 07:46:21     2021-09-15 07:44:45            -96.0   \n",
      "98      2021-09-15 07:50:00     2021-09-15 07:47:55           -125.0   \n",
      "99      2021-09-16 07:33:00     2021-09-16 07:33:10             10.0   \n",
      "\n",
      "   route_short_name  month  month_type day_type time_type  incremental_delay  \\\n",
      "0                 1      9           0  weekday   am_rush                0.0   \n",
      "1                 1      9           0  weekday   am_rush              -58.0   \n",
      "2                 1      9           0  weekday   am_rush                3.0   \n",
      "3                 1      9           0  weekday   am_rush              -54.0   \n",
      "4                 1      9           0  weekday   am_rush              -38.0   \n",
      "..              ...    ...         ...      ...       ...                ...   \n",
      "95                1      9           0  weekday   am_rush              -32.0   \n",
      "96                1      9           0  weekday   am_rush               -9.0   \n",
      "97                1      9           0  weekday   am_rush                6.0   \n",
      "98                1      9           0  weekday   am_rush              -29.0   \n",
      "99                1      9           0  weekday   am_rush                0.0   \n",
      "\n",
      "   scheduled_travel_time observed_travel_time  \n",
      "0        0 days 00:00:00      0 days 00:00:00  \n",
      "1        0 days 00:02:49      0 days 00:01:51  \n",
      "2        0 days 00:01:05      0 days 00:01:08  \n",
      "3        0 days 00:02:31      0 days 00:01:37  \n",
      "4        0 days 00:01:36      0 days 00:00:58  \n",
      "..                   ...                  ...  \n",
      "95       0 days 00:02:03      0 days 00:01:31  \n",
      "96       0 days 00:02:00      0 days 00:01:51  \n",
      "97       0 days 00:01:17      0 days 00:01:23  \n",
      "98       0 days 00:03:39      0 days 00:03:10  \n",
      "99       0 days 00:00:00      0 days 00:00:00  \n",
      "\n",
      "[100 rows x 18 columns]\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Set pandas display options globally\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', None)        # Don't wrap lines\n",
    "pd.set_option('display.max_colwidth', 50)   # Limit column width for readability\n",
    "\n",
    "print(test_form.form_data.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ANALYSIS FOR Djursjukhuset ===\n",
      "1. Basic counts:\n",
      "direction_id        0     1\n",
      "stop_id                    \n",
      "9022005003840001  918  1029\n",
      "9022005003840002  918  1029\n",
      "\n",
      "2. Trip overlap analysis:\n",
      "Trips at 9022005003840001: 57\n",
      "Trips at 9022005003840002: 57\n",
      "Trips at BOTH: 57\n",
      "Trips ONLY at stop1: 0\n",
      "Trips ONLY at stop2: 0\n",
      "\n",
      "3. Direction consistency:\n",
      "Trip 55700000059822245: directions [1], stop_ids ['9022005003840001' '9022005003840002']\n",
      "Trip 55700000059822593: directions [0], stop_ids ['9022005003840001' '9022005003840002']\n",
      "Trip 55700000059822277: directions [1], stop_ids ['9022005003840001' '9022005003840002']\n",
      "Trip 55700000059822625: directions [0], stop_ids ['9022005003840002' '9022005003840001']\n",
      "Trip 55700000059822309: directions [1], stop_ids ['9022005003840001' '9022005003840002']\n",
      "\n",
      "4. Stop sequence analysis:\n",
      "Trip 55700000059822245:\n",
      "                  stop_id  direction_id  stop_sequence            datetime\n",
      "28207    9022005003840001             1           11.0 2021-09-01 07:00:25\n",
      "28618    9022005003840002             1           15.0 2021-09-01 07:04:40\n",
      "140279   9022005003840001             1           11.0 2021-09-02 07:00:32\n",
      "140704   9022005003840002             1           15.0 2021-09-02 07:04:52\n",
      "252861   9022005003840001             1           11.0 2021-09-03 07:04:19\n",
      "253187   9022005003840002             1           15.0 2021-09-03 07:07:38\n",
      "488601   9022005003840001             1           11.0 2021-09-06 07:00:14\n",
      "489035   9022005003840002             1           15.0 2021-09-06 07:04:45\n",
      "600562   9022005003840001             1           11.0 2021-09-07 06:59:49\n",
      "601060   9022005003840002             1           15.0 2021-09-07 07:04:50\n",
      "710527   9022005003840001             1           11.0 2021-09-08 06:59:49\n",
      "711087   9022005003840002             1           15.0 2021-09-08 07:05:28\n",
      "820912   9022005003840001             1           11.0 2021-09-09 07:00:05\n",
      "821319   9022005003840002             1           15.0 2021-09-09 07:04:39\n",
      "931172   9022005003840001             1           11.0 2021-09-10 07:00:10\n",
      "931624   9022005003840002             1           15.0 2021-09-10 07:05:05\n",
      "1164382  9022005003840001             1           11.0 2021-09-13 07:00:38\n",
      "1164820  9022005003840002             1           15.0 2021-09-13 07:04:53\n",
      "1275796  9022005003840001             1           11.0 2021-09-14 07:00:27\n",
      "1276206  9022005003840002             1           15.0 2021-09-14 07:04:50\n",
      "1387182  9022005003840001             1           11.0 2021-09-15 07:00:18\n",
      "1387620  9022005003840002             1           15.0 2021-09-15 07:04:51\n",
      "1497743  9022005003840001             1           11.0 2021-09-16 07:00:01\n",
      "1498172  9022005003840002             1           15.0 2021-09-16 07:04:52\n",
      "1608438  9022005003840001             1           11.0 2021-09-17 07:00:04\n",
      "1608902  9022005003840002             1           15.0 2021-09-17 07:04:48\n",
      "1843072  9022005003840001             1           11.0 2021-09-20 07:00:11\n",
      "1843493  9022005003840002             1           15.0 2021-09-20 07:04:34\n",
      "1954372  9022005003840001             1           11.0 2021-09-21 07:00:49\n",
      "1954911  9022005003840002             1           15.0 2021-09-21 07:06:13\n",
      "Trip 55700000059822593:\n",
      "                  stop_id  direction_id  stop_sequence            datetime\n",
      "31781    9022005003840001             0           11.0 2021-09-01 07:38:38\n",
      "32311    9022005003840002             0           15.0 2021-09-01 07:43:38\n",
      "143990   9022005003840001             0           11.0 2021-09-02 07:40:12\n",
      "144323   9022005003840002             0           15.0 2021-09-02 07:43:32\n",
      "256371   9022005003840001             0           11.0 2021-09-03 07:41:34\n",
      "256816   9022005003840002             0           15.0 2021-09-03 07:45:55\n",
      "492258   9022005003840001             0           11.0 2021-09-06 07:39:07\n",
      "492656   9022005003840002             0           15.0 2021-09-06 07:43:27\n",
      "604300   9022005003840001             0           11.0 2021-09-07 07:39:55\n",
      "604716   9022005003840002             0           15.0 2021-09-07 07:44:15\n",
      "714243   9022005003840001             0           11.0 2021-09-08 07:39:53\n",
      "714622   9022005003840002             0           15.0 2021-09-08 07:43:28\n",
      "824493   9022005003840001             0           11.0 2021-09-09 07:38:25\n",
      "824941   9022005003840002             0           15.0 2021-09-09 07:43:19\n",
      "934820   9022005003840001             0           11.0 2021-09-10 07:39:56\n",
      "935263   9022005003840002             0           15.0 2021-09-10 07:44:13\n",
      "1168225  9022005003840001             0           11.0 2021-09-13 07:41:05\n",
      "1168538  9022005003840002             0           15.0 2021-09-13 07:44:25\n",
      "1279506  9022005003840001             0           11.0 2021-09-14 07:39:50\n",
      "1279847  9022005003840002             0           15.0 2021-09-14 07:43:30\n",
      "1390972  9022005003840001             0           11.0 2021-09-15 07:40:13\n",
      "1391344  9022005003840002             0           15.0 2021-09-15 07:44:13\n",
      "1500817  9022005003840001             0           11.0 2021-09-16 07:39:40\n",
      "1501185  9022005003840002             0           15.0 2021-09-16 07:43:20\n",
      "1612210  9022005003840001             0           11.0 2021-09-17 07:39:40\n",
      "1612599  9022005003840002             0           15.0 2021-09-17 07:43:58\n",
      "1846904  9022005003840001             0           11.0 2021-09-20 07:41:14\n",
      "1847308  9022005003840002             0           15.0 2021-09-20 07:45:14\n",
      "1958007  9022005003840001             0           11.0 2021-09-21 07:39:23\n",
      "1958559  9022005003840002             0           15.0 2021-09-21 07:44:37\n",
      "Trip 55700000059822277:\n",
      "                  stop_id  direction_id  stop_sequence            datetime\n",
      "33814    9022005003840001             1           11.0 2021-09-01 08:00:23\n",
      "34331    9022005003840002             1           15.0 2021-09-01 08:05:09\n",
      "145929   9022005003840001             1           11.0 2021-09-02 08:00:22\n",
      "146384   9022005003840002             1           15.0 2021-09-02 08:05:07\n",
      "258072   9022005003840001             1           11.0 2021-09-03 08:00:07\n",
      "258536   9022005003840002             1           15.0 2021-09-03 08:04:45\n",
      "494222   9022005003840001             1           11.0 2021-09-06 07:59:50\n",
      "494665   9022005003840002             1           15.0 2021-09-06 08:04:47\n",
      "606258   9022005003840001             1           11.0 2021-09-07 08:00:54\n",
      "606635   9022005003840002             1           15.0 2021-09-07 08:05:04\n",
      "716161   9022005003840001             1           11.0 2021-09-08 08:00:27\n",
      "716601   9022005003840002             1           15.0 2021-09-08 08:05:01\n",
      "826550   9022005003840001             1           11.0 2021-09-09 08:00:45\n",
      "826897   9022005003840002             1           15.0 2021-09-09 08:04:35\n",
      "936774   9022005003840001             1           11.0 2021-09-10 08:00:41\n",
      "937144   9022005003840002             1           15.0 2021-09-10 08:04:52\n",
      "1170060  9022005003840001             1           11.0 2021-09-13 08:00:27\n",
      "1170481  9022005003840002             1           15.0 2021-09-13 08:04:45\n",
      "1281491  9022005003840001             1           11.0 2021-09-14 08:00:16\n",
      "1281867  9022005003840002             1           15.0 2021-09-14 08:04:37\n",
      "1393122  9022005003840001             1           11.0 2021-09-15 08:03:01\n",
      "1393474  9022005003840002             1           15.0 2021-09-15 08:06:41\n",
      "1504257  9022005003840001             1           11.0 2021-09-16 08:17:51\n",
      "1504372  9022005003840002             1           15.0 2021-09-16 08:19:31\n",
      "1614112  9022005003840001             1           11.0 2021-09-17 08:00:07\n",
      "1614579  9022005003840002             1           15.0 2021-09-17 08:04:58\n",
      "1848681  9022005003840001             1           11.0 2021-09-20 08:00:24\n",
      "1849112  9022005003840002             1           15.0 2021-09-20 08:04:46\n",
      "1959919  9022005003840001             1           11.0 2021-09-21 07:59:56\n",
      "1960370  9022005003840002             1           15.0 2021-09-21 08:04:42\n",
      "\n",
      "=== ANALYSIS FOR Tingsvägen ===\n",
      "1. Basic counts:\n",
      "direction_id        0     1\n",
      "stop_id                    \n",
      "9022005003839001  918  1029\n",
      "9022005003839002  918  1029\n",
      "\n",
      "2. Trip overlap analysis:\n",
      "Trips at 9022005003839001: 57\n",
      "Trips at 9022005003839002: 57\n",
      "Trips at BOTH: 57\n",
      "Trips ONLY at stop1: 0\n",
      "Trips ONLY at stop2: 0\n",
      "\n",
      "3. Direction consistency:\n",
      "Trip 55700000059822245: directions [1], stop_ids ['9022005003839001' '9022005003839002']\n",
      "Trip 55700000059822593: directions [0], stop_ids ['9022005003839001' '9022005003839002']\n",
      "Trip 55700000059822277: directions [1], stop_ids ['9022005003839001' '9022005003839002']\n",
      "Trip 55700000059822625: directions [0], stop_ids ['9022005003839002' '9022005003839001']\n",
      "Trip 55700000059822309: directions [1], stop_ids ['9022005003839001' '9022005003839002']\n",
      "\n",
      "4. Stop sequence analysis:\n",
      "Trip 55700000059822245:\n",
      "                  stop_id  direction_id  stop_sequence            datetime\n",
      "28256    9022005003839001             1           12.0 2021-09-01 07:01:05\n",
      "28575    9022005003839002             1           14.0 2021-09-01 07:03:53\n",
      "140371   9022005003839001             1           12.0 2021-09-02 07:01:12\n",
      "140651   9022005003839002             1           14.0 2021-09-02 07:03:52\n",
      "252919   9022005003839001             1           12.0 2021-09-03 07:04:59\n",
      "253090   9022005003839002             1           14.0 2021-09-03 07:06:41\n",
      "488661   9022005003839001             1           12.0 2021-09-06 07:00:54\n",
      "488969   9022005003839002             1           14.0 2021-09-06 07:03:56\n",
      "600648   9022005003839001             1           12.0 2021-09-07 07:00:55\n",
      "600970   9022005003839002             1           14.0 2021-09-07 07:03:55\n",
      "710603   9022005003839001             1           12.0 2021-09-08 07:00:44\n",
      "711001   9022005003839002             1           14.0 2021-09-08 07:04:43\n",
      "820980   9022005003839001             1           12.0 2021-09-09 07:01:06\n",
      "821227   9022005003839002             1           14.0 2021-09-09 07:03:41\n",
      "931270   9022005003839001             1           12.0 2021-09-10 07:01:26\n",
      "931534   9022005003839002             1           14.0 2021-09-10 07:04:05\n",
      "1164482  9022005003839001             1           12.0 2021-09-13 07:01:39\n",
      "1164695  9022005003839002             1           14.0 2021-09-13 07:03:58\n",
      "1275845  9022005003839001             1           12.0 2021-09-14 07:01:11\n",
      "1276097  9022005003839002             1           14.0 2021-09-14 07:03:51\n",
      "1387275  9022005003839001             1           12.0 2021-09-15 07:01:20\n",
      "1387507  9022005003839002             1           14.0 2021-09-15 07:03:40\n",
      "1497819  9022005003839001             1           12.0 2021-09-16 07:01:01\n",
      "1498102  9022005003839002             1           14.0 2021-09-16 07:04:01\n",
      "1608489  9022005003839001             1           12.0 2021-09-17 07:00:43\n",
      "1608808  9022005003839002             1           14.0 2021-09-17 07:03:48\n",
      "1843149  9022005003839001             1           12.0 2021-09-20 07:01:15\n",
      "1843434  9022005003839002             1           14.0 2021-09-20 07:03:55\n",
      "1954446  9022005003839001             1           12.0 2021-09-21 07:01:35\n",
      "1954799  9022005003839002             1           14.0 2021-09-21 07:05:15\n",
      "Trip 55700000059822593:\n",
      "                  stop_id  direction_id  stop_sequence            datetime\n",
      "31898    9022005003839001             0           12.0 2021-09-01 07:39:38\n",
      "32145    9022005003839002             0           14.0 2021-09-01 07:42:18\n",
      "144054   9022005003839001             0           12.0 2021-09-02 07:40:52\n",
      "144255   9022005003839002             0           14.0 2021-09-02 07:42:52\n",
      "256448   9022005003839001             0           12.0 2021-09-03 07:42:15\n",
      "256708   9022005003839002             0           14.0 2021-09-03 07:44:55\n",
      "492322   9022005003839001             0           12.0 2021-09-06 07:39:48\n",
      "492536   9022005003839002             0           14.0 2021-09-06 07:42:08\n",
      "604379   9022005003839001             0           12.0 2021-09-07 07:40:35\n",
      "604609   9022005003839002             0           14.0 2021-09-07 07:43:15\n",
      "714291   9022005003839001             0           12.0 2021-09-08 07:40:28\n",
      "714544   9022005003839002             0           14.0 2021-09-08 07:42:48\n",
      "824569   9022005003839001             0           12.0 2021-09-09 07:39:25\n",
      "824861   9022005003839002             0           14.0 2021-09-09 07:42:22\n",
      "934914   9022005003839001             0           12.0 2021-09-10 07:40:53\n",
      "935148   9022005003839002             0           14.0 2021-09-10 07:43:13\n",
      "1168299  9022005003839001             0           12.0 2021-09-13 07:41:45\n",
      "1168458  9022005003839002             0           14.0 2021-09-13 07:43:45\n",
      "1279597  9022005003839001             0           12.0 2021-09-14 07:40:50\n",
      "1279732  9022005003839002             0           14.0 2021-09-14 07:42:30\n",
      "1391065  9022005003839001             0           12.0 2021-09-15 07:41:13\n",
      "1391254  9022005003839002             0           14.0 2021-09-15 07:43:13\n",
      "1500874  9022005003839001             0           12.0 2021-09-16 07:40:20\n",
      "1501092  9022005003839002             0           14.0 2021-09-16 07:42:20\n",
      "1612306  9022005003839001             0           12.0 2021-09-17 07:40:38\n",
      "1612467  9022005003839002             0           14.0 2021-09-17 07:42:18\n",
      "1846964  9022005003839001             0           12.0 2021-09-20 07:41:54\n",
      "1847242  9022005003839002             0           14.0 2021-09-20 07:44:34\n",
      "1958078  9022005003839001             0           12.0 2021-09-21 07:40:18\n",
      "1958438  9022005003839002             0           14.0 2021-09-21 07:43:37\n",
      "Trip 55700000059822277:\n",
      "                  stop_id  direction_id  stop_sequence            datetime\n",
      "33894    9022005003839001             1           12.0 2021-09-01 08:01:15\n",
      "34271    9022005003839002             1           14.0 2021-09-01 08:04:09\n",
      "146021   9022005003839001             1           12.0 2021-09-02 08:01:25\n",
      "146327   9022005003839002             1           14.0 2021-09-02 08:04:25\n",
      "258185   9022005003839001             1           12.0 2021-09-03 08:00:47\n",
      "258482   9022005003839002             1           14.0 2021-09-03 08:04:02\n",
      "494326   9022005003839001             1           12.0 2021-09-06 08:01:06\n",
      "494594   9022005003839002             1           14.0 2021-09-06 08:03:55\n",
      "606285   9022005003839001             1           12.0 2021-09-07 08:01:14\n",
      "606599   9022005003839002             1           14.0 2021-09-07 08:04:42\n",
      "716182   9022005003839001             1           12.0 2021-09-08 08:00:47\n",
      "716543   9022005003839002             1           14.0 2021-09-08 08:04:24\n",
      "826632   9022005003839001             1           12.0 2021-09-09 08:01:25\n",
      "826861   9022005003839002             1           14.0 2021-09-09 08:03:50\n",
      "936866   9022005003839001             1           12.0 2021-09-10 08:01:41\n",
      "937072   9022005003839002             1           14.0 2021-09-10 08:03:59\n",
      "1170146  9022005003839001             1           12.0 2021-09-13 08:01:26\n",
      "1170391  9022005003839002             1           14.0 2021-09-13 08:04:05\n",
      "1281557  9022005003839001             1           12.0 2021-09-14 08:01:00\n",
      "1281839  9022005003839002             1           14.0 2021-09-14 08:04:00\n",
      "1393168  9022005003839001             1           12.0 2021-09-15 08:03:41\n",
      "1393398  9022005003839002             1           14.0 2021-09-15 08:06:01\n",
      "1504284  9022005003839001             1           12.0 2021-09-16 08:18:31\n",
      "1504381  9022005003839002             1           14.0 2021-09-16 08:19:31\n",
      "1614190  9022005003839001             1           12.0 2021-09-17 08:01:00\n",
      "1614478  9022005003839002             1           14.0 2021-09-17 08:04:00\n",
      "1848768  9022005003839001             1           12.0 2021-09-20 08:01:04\n",
      "1849061  9022005003839002             1           14.0 2021-09-20 08:04:04\n",
      "1960025  9022005003839001             1           12.0 2021-09-21 08:00:56\n",
      "1960299  9022005003839002             1           14.0 2021-09-21 08:03:44\n"
     ]
    }
   ],
   "source": [
    "def analyze_contamination_pattern(df, stop_name):\n",
    "    \"\"\"\n",
    "    Deep dive into the contamination pattern for a specific stop\n",
    "    \"\"\"\n",
    "    stop_data = df[df['stop_name'] == stop_name].copy()\n",
    "    \n",
    "    print(f\"\\n=== ANALYSIS FOR {stop_name} ===\")\n",
    "    \n",
    "    # 1. Basic counts\n",
    "    print(\"1. Basic counts:\")\n",
    "    counts_by_stop_id = stop_data.groupby(['stop_id', 'direction_id']).size().unstack(fill_value=0)\n",
    "    print(counts_by_stop_id)\n",
    "    \n",
    "    # 2. Check if same trips appear at both stop_ids\n",
    "    print(\"\\n2. Trip overlap analysis:\")\n",
    "    stop_ids = stop_data['stop_id'].unique()\n",
    "    if len(stop_ids) == 2:\n",
    "        stop_id_1, stop_id_2 = stop_ids\n",
    "        \n",
    "        trips_at_stop1 = set(stop_data[stop_data['stop_id'] == stop_id_1]['trip_id'].unique())\n",
    "        trips_at_stop2 = set(stop_data[stop_data['stop_id'] == stop_id_2]['trip_id'].unique())\n",
    "        \n",
    "        print(f\"Trips at {stop_id_1}: {len(trips_at_stop1)}\")\n",
    "        print(f\"Trips at {stop_id_2}: {len(trips_at_stop2)}\")\n",
    "        print(f\"Trips at BOTH: {len(trips_at_stop1.intersection(trips_at_stop2))}\")\n",
    "        print(f\"Trips ONLY at stop1: {len(trips_at_stop1 - trips_at_stop2)}\")\n",
    "        print(f\"Trips ONLY at stop2: {len(trips_at_stop2 - trips_at_stop1)}\")\n",
    "    \n",
    "    # 3. Check direction consistency within trips\n",
    "    print(\"\\n3. Direction consistency:\")\n",
    "    for trip_id in stop_data['trip_id'].unique()[:5]:  # Check first 5 trips\n",
    "        trip_data = stop_data[stop_data['trip_id'] == trip_id]\n",
    "        directions = trip_data['direction_id'].unique()\n",
    "        stop_ids_in_trip = trip_data['stop_id'].unique()\n",
    "        print(f\"Trip {trip_id}: directions {directions}, stop_ids {stop_ids_in_trip}\")\n",
    "    \n",
    "    # 4. Check if there are different stop_sequences for same trip at different stop_ids\n",
    "    print(\"\\n4. Stop sequence analysis:\")\n",
    "    for trip_id in stop_data['trip_id'].unique()[:3]:\n",
    "        trip_data = stop_data[stop_data['trip_id'] == trip_id]\n",
    "        if len(trip_data) > 1:  # Trip appears multiple times at this stop\n",
    "            print(f\"Trip {trip_id}:\")\n",
    "            print(trip_data[['stop_id', 'direction_id', 'stop_sequence', 'datetime']].to_string())\n",
    "\n",
    "# Run the analysis for the problematic stops\n",
    "contaminated_stops = ['Djursjukhuset', 'Tingsvägen']\n",
    "for stop in contaminated_stops:\n",
    "    analyze_contamination_pattern(test_dataframes[226], stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City distribution:\n",
      "city\n",
      "Linköping    191492\n",
      "Name: count, dtype: int64\n",
      "Linköping stations: ['Tegskiftesgatan' 'Odalgatan' 'Bygdegatan' 'Hemmansgatan'\n",
      " 'Djurgården centrum' 'Infanterivägen' 'US Södra entrén' 'US Norra entrén'\n",
      " 'Tinnerbäcksbadet' 'Djurgården' 'Trädgårdstorget' 'Kungsgatan'\n",
      " 'Linköpings resecentrum' 'Garnisonen']\n",
      "Removing 0 duplicates.\n",
      "\n",
      "Found 13 potential stop pairs to process\n",
      "\n",
      "Verifying potentially problematic stops after corrections:\n",
      "  No remaining direction problems found in stop pairs.\n",
      "\n",
      "Direction distribution per stop (route order):\n",
      "direction_id                                0     1\n",
      "stop_name              stop_id                     \n",
      "Tegskiftesgatan        9022005000469002  7494     0\n",
      "Odalgatan              9022005004084001  7495     0\n",
      "Bygdegatan             9022005004076001  7496     0\n",
      "Hemmansgatan           9022005004250001  7497     0\n",
      "Djurgården centrum     9022005004251001  7497     0\n",
      "Djurgården             9022005004236002  7499     0\n",
      "Garnisonen             9022005004230002   885     0\n",
      "Infanterivägen         9022005000404002  6614     0\n",
      "US Södra entrén        9022005001249002  7501     0\n",
      "US Norra entrén        9022005000198002  7501     0\n",
      "Tinnerbäcksbadet       9022005000042002  7502     0\n",
      "Trädgårdstorget        9022005000046004  7502     0\n",
      "Kungsgatan             9022005001243002  6493     0\n",
      "Linköpings resecentrum 9022005000050006  7502  7363\n",
      "Kungsgatan             9022005001243001     0  6372\n",
      "Trädgårdstorget        9022005000046003     0  7363\n",
      "Tinnerbäcksbadet       9022005000042001     0  7364\n",
      "US Norra entrén        9022005000198001     0  7366\n",
      "US Södra entrén        9022005001249001     0  7366\n",
      "Garnisonen             9022005004230001     0   873\n",
      "Infanterivägen         9022005000404001     0  6493\n",
      "Djurgården             9022005004236001     0  7366\n",
      "Djurgården centrum     9022005004251002     0  7366\n",
      "Hemmansgatan           9022005004250002     0  7366\n",
      "Bygdegatan             9022005004076002     0  7366\n",
      "Odalgatan              9022005004084002     0  7366\n",
      "Tegskiftesgatan        9022005000469003     0  7366\n",
      "Memory optimization: 982.67 MB → 24.99 MB (97.5% reduction)\n"
     ]
    }
   ],
   "source": [
    "former_2 = DataFormer(test_2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City distribution:\n",
      "city\n",
      "Linköping    419861\n",
      "Name: count, dtype: int64\n",
      "Linköping stations: ['Hässlegatan' 'Myntgatan' 'Grindgatan' 'Resedan' 'Berga Söderleden'\n",
      " 'Ridhusgatan' 'Middagsgatan' 'Strågatan' 'Majelden' 'Rusthållaregården'\n",
      " 'Mellangården' 'Skäggetorp centrum' 'Blomgatan' 'Folkungavallen'\n",
      " 'Klostergatan' 'Åbylund' 'Abisko' 'Konsert & Kongress' 'Tinnerbäcksbadet'\n",
      " 'Trädgårdstorget' 'Kungsgatan' 'Linköpings resecentrum' 'Bergdalsgatan']\n",
      "Removing 0 duplicates.\n",
      "\n",
      "Found 19 potential stop pairs to process\n",
      "\n",
      "Verifying potentially problematic stops after corrections:\n",
      "  No remaining direction problems found in stop pairs.\n",
      "\n",
      "Direction distribution per stop (route order):\n",
      "direction_id                                0     1\n",
      "stop_name              stop_id                     \n",
      "Hässlegatan            9022005000044001  9459  9697\n",
      "Strågatan              9022005001229002  9460     0\n",
      "Middagsgatan           9022005001230002  9461     0\n",
      "Grindgatan             9022005001233002  9461     0\n",
      "Myntgatan              9022005001232002  9461     0\n",
      "Resedan                9022005000045002  9461     0\n",
      "Ridhusgatan            9022005001231002  9461     0\n",
      "Berga Söderleden       9022005000043002  9462     0\n",
      "Majelden               9022005001227002  9465     0\n",
      "Folkungavallen         9022005001221002  9469     0\n",
      "Tinnerbäcksbadet       9022005000042002  9471     0\n",
      "Trädgårdstorget        9022005000046004  9471     0\n",
      "Kungsgatan             9022005001243002  8194     0\n",
      "Linköpings resecentrum 9022005000050003  9476     0\n",
      "Klostergatan           9022005001223002  9476     0\n",
      "Konsert & Kongress     9022005000417003  9477     0\n",
      "Abisko                 9022005001219002  9477     0\n",
      "Åbylund                9022005001234002  9477     0\n",
      "Blomgatan              9022005001235002  9477     0\n",
      "Rusthållaregården      9022005001236003  9477     0\n",
      "Mellangården           9022005001411002  9477     0\n",
      "Skäggetorp centrum     9022005000048001  9477  9762\n",
      "Mellangården           9022005001411001     0  9762\n",
      "Rusthållaregården      9022005001236001     0  9763\n",
      "Blomgatan              9022005001235001     0  9763\n",
      "Åbylund                9022005001234001     0  9764\n",
      "Abisko                 9022005001219001     0  9765\n",
      "Konsert & Kongress     9022005000417001     0  9768\n",
      "Klostergatan           9022005001223001     0  9768\n",
      "Linköpings resecentrum 9022005000050002     0  9769\n",
      "Kungsgatan             9022005001243001     0  8385\n",
      "Trädgårdstorget        9022005000046003     0  9694\n",
      "Tinnerbäcksbadet       9022005000042001     0  9696\n",
      "Bergdalsgatan          9022005001222001     0  9696\n",
      "Majelden               9022005001227001     0  9696\n",
      "Berga Söderleden       9022005000043001     0  9696\n",
      "Ridhusgatan            9022005001231001     0  9696\n",
      "Resedan                9022005000045001     0  9696\n",
      "Myntgatan              9022005001232001     0  9696\n",
      "Grindgatan             9022005001233001     0  9696\n",
      "Middagsgatan           9022005001230001     0  9696\n",
      "Strågatan              9022005001229001     0  9696\n",
      "Memory optimization: 539.74 MB → 54.84 MB (89.8% reduction)\n"
     ]
    }
   ],
   "source": [
    "former_1 =DataFormer(test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City distribution:\n",
      "city\n",
      "Linköping    266034\n",
      "Name: count, dtype: int64\n",
      "Linköping stations: ['Ekdalsvägen' 'Lövsbergsvägen' 'Aspnäset' 'Vårdsbergsvägen'\n",
      " 'Hjulsbro skola' 'Roshagsvägen' 'Ljunghagsvägen' 'Vårgård'\n",
      " 'Hackefors södra' 'Hackefors' 'Landerydsvägen' 'Saab Södra' 'Saab Civila'\n",
      " 'Stationsgatan' 'Vetegatan' 'Tannefors center' 'Korngatan' 'Humlegatan'\n",
      " 'US Norra entrén' 'Tinnerbäcksgränd' 'Rotegatan' 'US Södra entrén'\n",
      " 'Pionjärgatan' 'Berga centrum' 'Tinnerbäcksbadet'\n",
      " 'Linköpings resecentrum' 'Kungsgatan' 'Trädgårdstorget']\n",
      "Removing 0 duplicates.\n",
      "\n",
      "Found 27 potential stop pairs to process\n",
      "\n",
      "Fixes for stop pair 'Hackefors':\n",
      "  Direction mapping: {'9022005001437002': np.int64(0), '9022005001437001': np.int64(1)}\n",
      "  - Fixed 555 records for stop ID 9022005001437001 -> 9022005001437002 (dir 0): 555\n",
      "\n",
      "Made a total of 555 corrections for stop pairs\n",
      "\n",
      "Verifying potentially problematic stops after corrections:\n",
      "  No remaining direction problems found in stop pairs.\n",
      "\n",
      "Direction distribution per stop (route order):\n",
      "direction_id                                0     1\n",
      "stop_name              stop_id                     \n",
      "Aspnäset               9022005000416001  4729  4743\n",
      "Lövsbergsvägen         9022005000226002  4730     0\n",
      "Ekdalsvägen            9022005001394002  4730     0\n",
      "Vårdsbergsvägen        9022005000437002  4730     0\n",
      "Hjulsbro skola         9022005001396001  4730     0\n",
      "Roshagsvägen           9022005001433001  4730     0\n",
      "Ljunghagsvägen         9022005000422001  4730     0\n",
      "Vårgård                9022005001435001  4730     0\n",
      "Hackefors södra        9022005001445001  4730     0\n",
      "Hackefors              9022005001437002  4730     0\n",
      "Landerydsvägen         9022005001421002  4731     0\n",
      "Saab Civila            9022005001419001  4733     0\n",
      "Saab Södra             9022005001418001  4733     0\n",
      "Stationsgatan          9022005001323001  4733     0\n",
      "Vetegatan              9022005001426001  4735     0\n",
      "Tannefors center       9022005000421001  4735     0\n",
      "Korngatan              9022005001424001  4735     0\n",
      "Humlegatan             9022005001422001  4736     0\n",
      "Linköpings resecentrum 9022005000050019  4814     0\n",
      "Kungsgatan             9022005001243001  4169     0\n",
      "Trädgårdstorget        9022005000046003  4816     0\n",
      "Tinnerbäcksbadet       9022005000042001  4816     0\n",
      "US Norra entrén        9022005000198001  4816     0\n",
      "US Södra entrén        9022005001249001  4816     0\n",
      "Tinnerbäcksgränd       9022005004186001  4816     0\n",
      "Rotegatan              9022005000412001  4816     0\n",
      "Pionjärgatan           9022005001431001  4816     0\n",
      "Berga centrum          9022005001430001  4816     0\n",
      "                       9022005001430002     0  4735\n",
      "Pionjärgatan           9022005001431002     0  4735\n",
      "Rotegatan              9022005000412002     0  4736\n",
      "Tinnerbäcksgränd       9022005004186002     0  4737\n",
      "US Södra entrén        9022005001249002     0  4737\n",
      "US Norra entrén        9022005000198002     0  4739\n",
      "Tinnerbäcksbadet       9022005000042002     0  4740\n",
      "Trädgårdstorget        9022005000046006     0  4740\n",
      "Kungsgatan             9022005001243002     0  4105\n",
      "Linköpings resecentrum 9022005000050018     0  4741\n",
      "Humlegatan             9022005001422002     0  4742\n",
      "Korngatan              9022005001424002     0  4742\n",
      "Tannefors center       9022005000421002     0  4743\n",
      "Vetegatan              9022005001426002     0  4743\n",
      "Stationsgatan          9022005001323002     0  4743\n",
      "Saab Södra             9022005001418002     0  4743\n",
      "Saab Civila            9022005001419002     0  4743\n",
      "Landerydsvägen         9022005001421001     0  4743\n",
      "Hackefors              9022005001437001     0  4743\n",
      "Hackefors södra        9022005001445002     0  4743\n",
      "Vårgård                9022005001435002     0  4743\n",
      "Ljunghagsvägen         9022005000422002     0  4743\n",
      "Roshagsvägen           9022005001433002     0  4743\n",
      "Hjulsbro skola         9022005001396002     0  4743\n",
      "Vårdsbergsvägen        9022005000437001     0  4743\n",
      "Ekdalsvägen            9022005001394001     0  4743\n",
      "Lövsbergsvägen         9022005000226001     0  4743\n",
      "Memory optimization: 341.55 MB → 34.61 MB (89.9% reduction)\n"
     ]
    }
   ],
   "source": [
    "former_16 = DataFormer(test_16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 1236 duplicates.\n",
      "\n",
      "Found 27 potential stop pairs to process\n",
      "\n",
      "Fixes for stop pair 'Hackefors':\n",
      "  Direction mapping: {'9022005001437001': np.int64(1), '9022005001437002': np.int64(0)}\n",
      "  - Fixed 555 records for stop ID 9022005001437001 -> 9022005001437002 (dir 0): 555\n",
      "\n",
      "Made a total of 555 corrections for stop pairs\n",
      "\n",
      "Verifying potentially problematic stops after corrections:\n",
      "  No remaining direction problems found in stop pairs.\n",
      "Memory optimization: 341.55 MB → 34.35 MB (89.9% reduction)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_form_data = former.final_form_data[former.final_form_data['start_date'] < '2024-01-01'].copy()\n",
    "evaluation_form_data = former.final_form_data[former.final_form_data['start_date'] >= '2024-01-01'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(former.clust_res_dep)\n",
    "print(former.clust_res_inc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(former.final_form_data.iloc[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_cols_ord=['stop_id', 'stop_name','direction_id', 'month_type_ord', 'day_type', 'time_type']\n",
    "group_cols_dep=['stop_id', 'stop_name','direction_id', 'month_type_dep', 'day_type', 'time_type']\n",
    "group_cols_inc=['stop_id', 'stop_name','direction_id', 'month_type_inc', 'day_type', 'time_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ana_ord = DataAnalyzer(form_data=training_form_data, group_cols=group_cols_ord)\n",
    "ana_dep = DataAnalyzer(form_data=training_form_data, group_cols=group_cols_dep)\n",
    "ana_inc = DataAnalyzer(form_data=training_form_data, group_cols=group_cols_inc)\n",
    "\n",
    "percentiles = np.arange(10, 100, 5)\n",
    "\n",
    "ana_ord.do_route_analysis(percentiles=percentiles)\n",
    "ana_ord.save_analysis_and_form_data_to_csv(filename_ana= 'ana_ord.csv')\n",
    "\n",
    "ana_dep.do_route_analysis(percentiles=percentiles)\n",
    "ana_dep.save_analysis_and_form_data_to_csv(filename_ana= 'ana_dep.csv')\n",
    "\n",
    "ana_inc.do_route_analysis(percentiles=percentiles)\n",
    "ana_inc.save_analysis_and_form_data_to_csv(filename_ana= 'ana_inc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_for_training_ord = DataAnalyzer()\n",
    "analyzer_for_training_dep = DataAnalyzer()\n",
    "analyzer_for_training_inc = DataAnalyzer()\n",
    "analyzer_for_training_ord.load_analysis_and_form_data_from_csv(filename_ana='ana_ord.csv')\n",
    "analyzer_for_training_dep.load_analysis_and_form_data_from_csv(filename_ana='ana_dep.csv')\n",
    "analyzer_for_training_inc.load_analysis_and_form_data_from_csv(filename_ana='ana_inc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in analyzer_for_training_inc._route_analysis.columns:\n",
    "    print(col, analyzer_for_training_inc._route_analysis[col].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The following cells should be ran with the ScheduleOptimizer set with the too early too late setting that se are interested in'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create and run the optimizer\n",
    "optimizer_for_evaluation_ord = ScheduleOptimizer(evaluation_form_data.copy(), analyzer_for_training_ord)\n",
    "optimizer_for_evaluation_dep = ScheduleOptimizer(evaluation_form_data.copy(), analyzer_for_training_dep)\n",
    "optimizer_for_evaluation_inc = ScheduleOptimizer(evaluation_form_data.copy(), analyzer_for_training_inc)\n",
    "\n",
    "# Define parameter combinations to test\n",
    "clust_types = [optimizer_for_evaluation_ord, optimizer_for_evaluation_dep, optimizer_for_evaluation_inc]\n",
    "delay_types = ['departure_delay', 'incremental_delay']\n",
    "percentiles = percentiles  # Using your previously defined percenti_les\n",
    "scaling_factors = np.arange(0.1, 1.0, 0.1)\n",
    "stat_methods = ['mean', 'median']  # Add mean and median as optimization methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in clust_types:\n",
    "    for col in elem.enriched_data.columns:\n",
    "        print(col, elem.enriched_data[col].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results with clear identifiers\n",
    "all_summaries = {}\n",
    "\n",
    "# Process percentile-based methods\n",
    "for clust in clust_types:\n",
    "    for delay_type in delay_types:\n",
    "        for percentile in percentiles:\n",
    "            for scaling_factor in scaling_factors:\n",
    "                # Create identifier for this combination\n",
    "                key = f\"{clust}_{delay_type}_percentile_{percentile}_sf{scaling_factor}\"\n",
    "                \n",
    "                # Run optimization\n",
    "                clust.find_new_departure_times(\n",
    "                    delay_type=delay_type,\n",
    "                    opt_method=f'percentile_{percentile}',\n",
    "                    scaling_factor=scaling_factor\n",
    "                )\n",
    "                \n",
    "                # Get and store summary\n",
    "                summary = clust.get_optimization_summary(\n",
    "                    delay_type=delay_type,\n",
    "                    opt_method=f'percentile_{percentile}',\n",
    "                    scaling_factor=scaling_factor\n",
    "                )\n",
    "                \n",
    "                all_summaries[key] = summary\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process mean and median methods\n",
    "for clust in clust_types:\n",
    "    for delay_type in delay_types:\n",
    "        for stat_method in stat_methods:\n",
    "            for scaling_factor in scaling_factors:\n",
    "                # Create identifier for this combination\n",
    "                key = f\"{clust}_{delay_type}_{stat_method}_sf{scaling_factor}\"\n",
    "                \n",
    "                # Run optimization\n",
    "                clust.find_new_departure_times(\n",
    "                    delay_type=delay_type,\n",
    "                    opt_method=stat_method,\n",
    "                    scaling_factor=scaling_factor\n",
    "                )\n",
    "                \n",
    "                # Get and store summary\n",
    "                summary = clust.get_optimization_summary(\n",
    "                    delay_type=delay_type,\n",
    "                    opt_method=stat_method,\n",
    "                    scaling_factor=scaling_factor\n",
    "                )\n",
    "                \n",
    "                all_summaries[key] = summary\n",
    "\n",
    "# Count total optimizations performed\n",
    "total_optimizations = len(all_summaries)\n",
    "print(f\"Completed {total_optimizations} different optimization strategies\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A more useful way to examine results\n",
    "best_improvement = None\n",
    "best_config = None\n",
    "best_value = float('-inf')\n",
    "\n",
    "for key, summary in all_summaries.items():\n",
    "    # Get on-time percentage improvement\n",
    "    on_time_row = summary[summary['metric'] == 'On-Time %']\n",
    "    if not on_time_row.empty:\n",
    "        improvement = on_time_row['percent_improvement'].values[0]\n",
    "        if pd.notnull(improvement) and improvement > best_value:\n",
    "            best_value = improvement\n",
    "            best_config = key\n",
    "            best_improvement = on_time_row\n",
    "\n",
    "print(f\"Best configuration: {best_config}\")\n",
    "print(f\"On-time percentage improvement: {best_value:.2f}%\")\n",
    "print(\"Best summary:\")\n",
    "print(all_summaries[best_config])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a list to hold all summary DataFrames with strategy info added\n",
    "all_dfs = []\n",
    "\n",
    "for strategy_key, summary_df in all_summaries.items():\n",
    "    # Add strategy information as columns\n",
    "    summary_df = summary_df.copy()\n",
    "    summary_df['strategy'] = strategy_key\n",
    "    \n",
    "    # Extract strategy components more safely\n",
    "    if 'incremental_delay' in strategy_key:\n",
    "        summary_df['delay_type'] = 'incremental_delay'\n",
    "    elif 'departure_delay' in strategy_key:\n",
    "        summary_df['delay_type'] = 'departure_delay'\n",
    "    else:\n",
    "        summary_df['delay_type'] = 'unknown'\n",
    "        \n",
    "    # Extract method (mean, median, percentile)\n",
    "    if 'mean' in strategy_key:\n",
    "        summary_df['method'] = 'mean'\n",
    "    elif 'median' in strategy_key:\n",
    "        summary_df['method'] = 'median'\n",
    "    elif 'percentile' in strategy_key:\n",
    "        # Extract percentile value if possible\n",
    "        import re\n",
    "        percentile_match = re.search(r'percentile_(\\d+)', strategy_key)\n",
    "        if percentile_match:\n",
    "            summary_df['method'] = f\"percentile_{percentile_match.group(1)}\"\n",
    "        else:\n",
    "            summary_df['method'] = 'percentile'\n",
    "    else:\n",
    "        summary_df['method'] = 'unknown'\n",
    "        \n",
    "    # Extract scaling factor\n",
    "    import re\n",
    "    sf_match = re.search(r'sf([\\d\\.]+)', strategy_key)\n",
    "    if sf_match:\n",
    "        summary_df['scaling_factor'] = float(sf_match.group(1))\n",
    "    else:\n",
    "        summary_df['scaling_factor'] = None\n",
    "    \n",
    "    all_dfs.append(summary_df)\n",
    "\n",
    "# Combine all DataFrames\n",
    "combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "# Save to CSV\n",
    "combined_df.to_csv('summaries_30to90.csv', index=False)\n",
    "\n",
    "print(f\"Saved combined summary with {len(combined_df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "combined_summaries = pd.read_csv('summaries_30to200.csv')\n",
    "print(combined_summaries)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
